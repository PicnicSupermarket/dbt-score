{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to dbt-score","text":"<p><code>dbt-score</code> is a linter for dbt metadata.</p> <p>dbt allows data practitioners to organize their data into models and sources. Those models and sources have metadata associated with them: documentation, tests, types, etc. dbt supports more types of entities, e.g. snapshots, analysis, seeds and more.</p> <p><code>dbt-score</code> allows to lint and score this metadata, in order to enforce (or encourage) good practices. The dbt entities that <code>dbt-score</code> is able to lint (currently) are:</p> <ul> <li>Models</li> <li>Sources</li> <li>Snapshots</li> <li>Exposures</li> <li>Seeds</li> </ul>"},{"location":"#example","title":"Example","text":"<pre><code>&gt; dbt-score lint --show all\n\ud83e\udd47 M: customers (score: 10.0)\n    OK   dbt_score.rules.generic.has_description\n    OK   dbt_score.rules.generic.has_owner\n    OK   dbt_score.rules.generic.sql_has_reasonable_number_of_lines\nScore: 10.0 \ud83e\udd47\n</code></pre> <p>In this example, the model <code>customers</code> scores the maximum value of <code>10.0</code> as it passes all the rules. It also is awarded a golden medal because of the perfect score. By default a passing dbt entity with or without rule violations will not be shown, unless we pass the <code>--show-all</code> flag.</p>"},{"location":"#philosophy","title":"Philosophy","text":"<p>dbt entities are often used as metadata containers: either in YAML files or through the use of <code>{{ config() }}</code> blocks, they are associated with a lot of information. At scale, it becomes tedious to enforce good practices in large data teams dealing with many dbt entities.</p> <p>To that end, <code>dbt-score</code> has 2 main features:</p> <ul> <li>It runs rules on dbt entities, and displays any rule violations. These can be   used in interactive environments or in CI.</li> <li>Using those run results, it scores items, to ascribe them a measure of their   maturity. This score can help gamify metadata improvements/coverage, and be   reflected in data catalogs.</li> </ul> <p><code>dbt-score</code> aims to:</p> <ul> <li>Provide a predefined set of good practices (the core rules).</li> <li>Allow teams to easily add their own rules.</li> <li>Allow rule sets to be packaged and distributed.</li> <li>Be configurable to adapt to different data stacks and practices.</li> </ul>"},{"location":"#about","title":"About","text":"<p><code>dbt-score</code> is free software, released under the MIT license. It originated at Picnic Technologies in Amsterdam, Netherlands. Source code is available on Github.</p> <p>All contributions, in the form of bug reports, pull requests, feedback or discussion are welcome. See the contribution guide for more information.</p>"},{"location":"configuration/","title":"Configuration","text":"<p><code>dbt-score</code> can be configured through a <code>pyproject.toml</code> file or via the command line.</p>"},{"location":"configuration/#pyprojecttoml","title":"pyproject.toml","text":"<p>It is recommended to place the file in the root of your dbt project. <code>dbt-score</code> will look for <code>pyproject.toml</code> in the directory from which it is run and its parent directories.</p> <p>An example of a <code>pyproject.toml</code> file to configure <code>dbt-score</code> can be found below:</p> <pre><code>[tool.dbt-score]\nrule_namespaces = [\"dbt_score.rules\", \"dbt_score_rules\", \"custom_rules\"]\ndisabled_rules = [\"dbt_score.rules.generic.columns_have_description\"]\ninject_cwd_in_python_path = true\nfail_project_under = 7.5\nfail_any_item_under = 8.0\n\n[tool.dbt-score.badges]\nfirst.threshold = 10.0\nfirst.icon = \"\ud83e\udd47\"\nsecond.threshold = 8.0\nsecond.icon = \"\ud83e\udd48\"\nthird.threshold = 6.0\nthird.icon = \"\ud83e\udd49\"\nwip.icon = \"\ud83c\udfd7\ufe0f\"\n\n[tool.dbt-score.rules.\"dbt_score.rules.generic.sql_has_reasonable_number_of_lines\"]\nseverity = 1\nmax_lines = 300\n</code></pre>"},{"location":"configuration/#configuration-options","title":"Configuration options","text":"<p>The following options can be set in the <code>pyproject.toml</code> file:</p>"},{"location":"configuration/#main-configuration","title":"Main configuration","text":"<pre><code>[tool.dbt-score]\n</code></pre> <ul> <li><code>rule_namespaces</code>: A list of Python namespaces to search for rules. The   default is <code>[\"dbt_score.rules\", \"dbt_score_rules\"]</code>. Be aware when overriding   this setting, that the default rules are in <code>dbt_score.rules</code> and are disabled   if not included here.</li> <li><code>disabled_rules</code>: A list of rules to disable.</li> <li><code>fail_project_under</code> (default: <code>5.0</code>): If the project score is below this   value the command will fail with return code 1.</li> <li><code>fail_any_item_under</code> (default: <code>5.0</code>): If any entity scores below this value   the command will fail with return code 1.</li> </ul>"},{"location":"configuration/#badges-configuration","title":"Badges configuration","text":"<pre><code>[tool.dbt-score.badges]\n</code></pre> <p>Four badges can be configured: <code>first</code>, <code>second</code>, <code>third</code> and <code>wip</code>. Each badge can be configured with the following option:</p> <ul> <li><code>icon</code>: The icon to use for the badge. A string that will be displayed in the   output, e.g. <code>\ud83e\udd47</code>.</li> </ul> <p>All badges except <code>wip</code> can be configured with the following option:</p> <ul> <li><code>threshold</code>: The threshold for the badge. A decimal number between <code>0.0</code> and   <code>10.0</code> that will be used to compare to the score. The threshold is the minimum   score required for a model or source to be rewarded with a certain badge.</li> </ul> <p>The default values can be found in the BadgeConfig.</p>"},{"location":"configuration/#rule-configuration","title":"Rule configuration","text":"<pre><code>[tool.dbt-score.rules.\"rule_namespace.rule_name\"]\n</code></pre> <p>Every rule can be configured with the following option:</p> <ul> <li><code>severity</code>: The severity of the rule. Rules have a default severity and can be   overridden. It's an integer with a minimum value of 1 and a maximum value   of 4.</li> <li><code>rule_filter_names</code>: Filters used by the rule. Takes a list of names that can   be found in the same namespace as the rules (see   Package rules).</li> </ul> <p>Example: the generic rule <code>has_example_sql</code> shall apply only to models   materializing a table.</p> <pre><code>[tool.dbt-score.rules.\"dbt_score.rules.generic.has_example_sql\"]\nrule_filter_names=[\"dbt_score.rules.filters.is_table\"]\n</code></pre> <p>Some rules have additional configuration options, e.g. sql_has_reasonable_number_of_lines. Depending on the rule, the options will have different names, types and default values. In the case of the sql_has_reasonable_number_of_lines, the <code>max_lines</code> option can be configured.</p>"},{"location":"configuration/#command-line","title":"Command line","text":"<p>Many configuration options can also be set via the command line. To understand how to configure <code>dbt-score</code> from the command line:</p> <pre><code>dbt-score lint --help\n</code></pre>"},{"location":"contributing/","title":"Contributor's guide","text":"<p><code>dbt-score</code> is free software, your contributions are welcome! \ud83d\ude80</p>"},{"location":"contributing/#reporting-bugs","title":"Reporting bugs","text":"<p>If you encountered a bug, check the issue tracker on Github to see if it is already known. If not, feel free to open a new issue. Share all relevant details, especially how to reproduce the problem.</p> <p>We'd love to hear from you and help make <code>dbt-score</code> as stable as we can.</p>"},{"location":"contributing/#developing-new-features","title":"Developing new features","text":""},{"location":"contributing/#adding-rules","title":"Adding rules","text":"<p>The linting rules bundled with <code>dbt-score</code> aim to be as generic as possible, and apply to a large majority of dbt projects.</p> <p>This is why they are not very opinionated: for example, we believe documenting data models is important, and hope it's not a controversial opinion ;) Of course, it's always possible to disable any rule.</p> <p>If you think a new rule should be created in <code>dbt-score</code>, feel free to open an issue to discuss it first - this might save you some work in case maintainers don't see a fit.</p> <p>If your rule idea is not generic and applies to your own project and logic, <code>dbt-score</code> has been designed to fully support custom rules. Create as many as you need for your purposes!</p>"},{"location":"contributing/#fixing-bugs","title":"Fixing bugs","text":"<p>We love bug squashing! You can open a pull request to fix any bug you encounter in <code>dbt-score</code>. If the changes are large enough, refer to the next section first - discussing a solution in a Github issue is always a good idea to avoid unnecessary work and orchestrate efforts.</p>"},{"location":"contributing/#adding-or-changing-core-features","title":"Adding or changing core features","text":"<p>Before implementing or changing a new feature, we kindly ask you to open a Github issue to get the maintainers' opinion on that feature. It might have been already considered, discussed, or already in the works.</p> <p>We aim to maintain a high code coverage in <code>dbt-score</code>'s unit tests, so new features should be properly tested for happy and unhappy paths.</p> <p>If the feature has direct impact on users, it should also be reflected in the documentation website.</p>"},{"location":"contributing/#development-environment","title":"Development environment","text":""},{"location":"contributing/#prerequisites","title":"Prerequisites","text":"<p>You'll need the following:</p> <ul> <li>Any Python version starting from 3.10</li> <li>pre-commit (recommended)</li> <li>uv</li> </ul> <p>After cloning the repository with git, configure your development environment by running these commands from the project's root:</p> <pre><code>pre-commit install\nuv sync --all-groups\n</code></pre> <p>The uv command will install all project's dependency groups, including all the dependencies needed for development purposes.</p>"},{"location":"contributing/#lint","title":"Lint","text":"<p><code>dbt-score</code> uses:</p> <ul> <li>ruff for fast linting and formatting.</li> <li>mypy for type checking.</li> <li>pre-commit-hooks.</li> <li>prettier-hooks.</li> </ul> <p>Cheatsheet:</p> <pre><code>uv run ruff check .\nuv run ruff check --fix\nuv run mypy .\nuv run tox -e lint\n</code></pre>"},{"location":"contributing/#test","title":"Test","text":"<p><code>dbt-score</code> uses:</p> <ul> <li>pytest as a main test framework.</li> <li>coverage for test   coverage.</li> <li>tox for testing against multiple Python   versions.</li> </ul> <p>Cheatsheet:</p> <pre><code>uv run tox -e py\nuv run pytest\nuv run coverage run -m pytest\n</code></pre>"},{"location":"contributing/#docs","title":"Docs","text":"<p><code>dbt-score</code> uses:</p> <ul> <li>mkdocs for docs generation.</li> <li>mkdocstrings for automatic docs from   sources.</li> </ul> <p>Cheatsheet:</p> <pre><code>uv run mkdocs build\nuv run mkdocs serve\n</code></pre>"},{"location":"contributing/#pre-commit","title":"Pre-commit","text":"<p>Cheatsheet:</p> <p>Execute hooks manually:</p> <pre><code>pre-commit run --all-files\n</code></pre> <p>Create a commit bypassing hooks:</p> <pre><code>git commit --no-verify\n</code></pre>"},{"location":"create_rules/","title":"Create rules","text":"<p>In order to lint and score dbt entities, <code>dbt-score</code> uses a set of rules that are applied to each item. A rule can pass or fail when it is run. Based on the severity of the rule, items are scored with the weighted average of the rules results. Note that <code>dbt-score</code> comes bundled with a set of default rules.</p> <p>On top of the generic rules, it's possible to add your own rules. Two ways exist to create a new rule:</p> <ol> <li>By using the <code>@rule</code> decorator, which is the preferred/simple way.</li> <li>By inheriting from the <code>Rule</code> class, which is more advanced and allows to    keep state between evaluations.</li> </ol>"},{"location":"create_rules/#using-the-rule-decorator","title":"Using the <code>@rule</code> decorator","text":"<p>The <code>@rule</code> decorator can be used to easily create a new rule:</p> <pre><code>from dbt_score import Model, rule, RuleViolation\n\n@rule\ndef model_has_description(model: Model) -&gt; RuleViolation | None:\n    \"\"\"A model should have a description.\"\"\"\n    if not model.description:\n        return RuleViolation(message=\"Model lacks a description.\")\n</code></pre> <p>The name of the function is the name of the rule and the docstring of the function is its description. Therefore, it is important to use a self-explanatory name for the function and document it well.</p> <p>The type annotation for the rule's argument dictates whether the rule should be applied to dbt models, sources, snapshots, seeds, or exposures.</p> <p>Here is the same example rule, applied to sources:</p> <pre><code>from dbt_score import rule, RuleViolation, Source\n\n@rule\ndef source_has_description(source: Source) -&gt; RuleViolation | None:\n   \"\"\"A source should have a description.\"\"\"\n   if not source.description:\n      return RuleViolation(message=\"Source lacks a description.\")\n</code></pre> <p>The severity of a rule can be set using the <code>severity</code> argument:</p> <pre><code>from dbt_score import rule, Severity\n\n@rule(severity=Severity.HIGH)\n</code></pre>"},{"location":"create_rules/#using-the-rule-class","title":"Using the <code>Rule</code> class","text":"<p>For more advanced use cases, a rule can be created by inheriting from the <code>Rule</code> class:</p> <pre><code>from dbt_score import Model, Rule, RuleViolation, Source\n\nclass ModelHasDescription(Rule):\n    description = \"A model should have a description.\"\n\n    def evaluate(self, model: Model) -&gt; RuleViolation | None:\n        \"\"\"Evaluate the rule.\"\"\"\n        if not model.description:\n            return RuleViolation(message=\"Model lacks a description.\")\n\nclass SourceHasDescription(Rule):\n   description = \"A source should have a description.\"\n\n   def evaluate(self, source: Source) -&gt; RuleViolation | None:\n      \"\"\"Evaluate the rule.\"\"\"\n      if not source.description:\n         return RuleViolation(message=\"Source lacks a description.\")\n</code></pre>"},{"location":"create_rules/#rules-location","title":"Rules location","text":"<p>By default <code>dbt-score</code> will look for rules in the Python namespace <code>dbt_score_rules</code>. Rules can be stored anywhere in the Python path under the <code>dbt_score_rules</code> namespace. In many cases, having such a folder in the dbt project from where you invoke dbt and dbt-score will work. In this folder, all rules defined in <code>.py</code> files will be automatically discovered. By default, the current working directory is injected in the Python path.</p> <p>If nested folders are used, e.g. <code>dbt_score_rules/generic_rules/rules.py</code>, an <code>__init__.py</code> file needs to be present in the nested folder to make it discoverable.</p>"},{"location":"create_rules/#configurable-rules","title":"Configurable rules","text":"<p>It's possible to create rules that can be configured with parameters . In order to create a configurable rule, the evaluation function of the rule should have additional input parameters with a default value defined. In the example below, the rule has a <code>max_lines</code> parameter with a default value of 200, which can be configured in the <code>pyproject.toml</code> file.</p> <pre><code>from dbt_score import Model, rule, RuleViolation\n\n@rule\ndef sql_has_reasonable_number_of_lines(model: Model, max_lines: int = 200) -&gt; RuleViolation | None:\n    \"\"\"The SQL query of a model should not be too long.\"\"\"\n    count_lines = len(model.raw_code.split(\"\\n\"))\n    if count_lines &gt; max_lines:\n        return RuleViolation(\n            message=f\"SQL query too long: {count_lines} lines (&gt; {max_lines}).\"\n        )\n</code></pre>"},{"location":"create_rules/#filtering-rules","title":"Filtering rules","text":"<p>Custom and standard rules can be configured to have filters. Filters allow a dbt entity to be ignored by one or multiple rules if the item doesn't satisfy the filter criteria.</p> <p>Filters are created using the same discovery mechanism and interface as custom rules, except they do not accept parameters. Similar to Python's built-in <code>filter</code> function, when the filter evaluation returns <code>True</code> the item should be evaluated, otherwise it should be ignored.</p> <pre><code>from dbt_score import Model, RuleFilter, rule_filter\n\n@rule_filter\ndef only_schema_x(model: Model) -&gt; bool:\n    \"\"\"Only applies a rule to schema X.\"\"\"\n    return model.schema.lower() == 'x'\n\nclass SkipSchemaY(RuleFilter):\n    description = \"Applies a rule to every schema but Y.\"\n    def evaluate(self, model: Model) -&gt; bool:\n      return model.schema.lower() != 'y'\n</code></pre> <p>Filters also rely on type-annotations to dictate whether they apply to models, sources, snapshots, seeds, or exposures:</p> <pre><code>from dbt_score import RuleFilter, rule_filter, Source\n\n@rule_filter\ndef only_from_source_a(source: Source) -&gt; bool:\n   \"\"\"Only applies a rule to source tables from source X.\"\"\"\n   return source.source_name.lower() == 'a'\n\nclass SkipSourceDatabaseB(RuleFilter):\n   description = \"Applies a rule to every source except Database B.\"\n   def evaluate(self, source: Source) -&gt; bool:\n      return source.database.lower() != 'b'\n</code></pre> <p>Similar to setting a rule severity, standard rules can have filters set in the configuration file, while custom rules accept the configuration file or a decorator parameter.</p> <pre><code>from dbt_score import Model, rule, RuleViolation\nfrom my_project import only_schema_x\n\n@rule(rule_filters={only_schema_x()})\ndef models_in_x_follow_naming_standard(model: Model) -&gt; RuleViolation | None:\n    \"\"\"Models in schema X must follow the naming standard.\"\"\"\n    if some_regex_fails(model.name):\n        return RuleViolation(\"Invalid model name.\")\n</code></pre>"},{"location":"create_rules/#debugging-rules","title":"Debugging rules","text":"<p>When writing new rules, or investigating failing ones, you can make use of a debug mode, which will automatically give you a debugger in case of an exception occurring.</p> <p>Run dbt-score with the debugger:</p> <pre><code>dbt-score lint --debug\n# --debug and -d are equivalent\n</code></pre> <p>The debugger is the standard <code>pdb</code>, see its available commands.</p> <p>Naturally, you're free to use your debugger of choice, this option exists to enable quick debugging in any environment.</p>"},{"location":"get_started/","title":"Get started","text":"<p><code>dbt-score</code> is a Python library that is easy to install and use. The minimum required version of Python is <code>3.10</code>.</p>"},{"location":"get_started/#installation","title":"Installation","text":"<p>Installation of <code>dbt-score</code> is simple:</p> <pre><code>pip install dbt-score\n</code></pre> <p>In order to run <code>dbt-score</code> with all its features, be sure to install <code>dbt-score</code> in the same environment as <code>dbt-core</code>.</p>"},{"location":"get_started/#usage","title":"Usage","text":"<p><code>dbt-score</code> uses the output of <code>dbt parse</code> (<code>manifest.json</code>) as input. Therefore, it is recommended to run <code>dbt-score</code> from the root of your dbt project. By default, it will look for <code>manifest.json</code> in dbt's <code>target</code> directory.</p> <p><code>dbt-score</code> can be executed from the command line:</p> <pre><code>dbt-score lint\n</code></pre> <p>To use a different manifest file, use the <code>--manifest</code> option:</p> <pre><code>dbt-score lint --manifest path/to/manifest.json\n</code></pre> <p>It's also possible to automatically run <code>dbt parse</code>, to generate the <code>manifest.json</code> file:</p> <pre><code>dbt-score lint --run-dbt-parse\n</code></pre> <p>To lint only a selection of dbt entities, the argument <code>--select</code> can be used. It accepts any dbt node selection syntax:</p> <pre><code>dbt-score lint --select +my_model+\n</code></pre> <p>To get more information on how to run <code>dbt-score</code>, <code>--help</code> can be used:</p> <pre><code>dbt-score --help\n</code></pre> <pre><code>dbt-score lint --help\n</code></pre>"},{"location":"package_rules/","title":"Package rules","text":"<p><code>dbt-score</code> is able to search for rules in Python packages.</p> <p>The default namespaces which are searched are:</p> <ul> <li><code>dbt_score.rules</code>: This namespace contains the core rules, i.e. those packaged   within <code>dbt-score</code>.</li> <li><code>dbt_score_rules</code>: This implicit namespace package can be used for any custom   rule, either implemented by 3rd party packages or by yourself.</li> </ul> <p>Additionally, <code>dbt-score</code> can be configured to search for rules in other namespaces.</p>"},{"location":"package_rules/#packaging-for-a-single-project","title":"Packaging for a single project","text":"<p>If you want to write custom rules applicable for your project only, it is recommended to bundle them directly within your dbt project.</p> <p>The only requirement for <code>dbt-score</code> to discover your custom rules is for those rules to be present and importable in your Python environment, which might vary depending on the way you use Python and virtual environments.</p> <p>The following project structure is usually observed:</p> <pre><code>my-dbt-project/\n\u251c\u2500 dbt_score_rules/\n\u2502  \u251c\u2500 my_project_rules.py\n\u251c\u2500 dbt_project.yml\n\u251c\u2500 models/\n\u251c\u2500 ...\n</code></pre> <p>For your convenience, <code>dbt-score</code> will inject the current working directory in the Python path, making <code>dbt_score_rules</code> discoverable here without any further configuration.</p>"},{"location":"package_rules/#packaging-for-multiple-projects","title":"Packaging for multiple projects","text":"<p>If you want to write custom rules which apply to multiple dbt projects within an organization, or even to be distributed to the public, the best way is to package them in a Python wheel.</p> <p>The only thing the wheel needs to do is to expose modules within a top-level <code>dbt_score_rules</code> (or any other namespace, as long as projects are configured to read from such an additional namespace).</p> <p>To avoid naming conflicts within the <code>dbt_score_rules</code> namespace, it is recommended to pick module names which match either an organization, or a specific project. For example, an hypothetical <code>dbtviz</code> project which makes use of dbt's <code>meta</code> to describe visualizations associated to models could have rules for this metadata saved in <code>dbt_score_rules.dbtviz</code>.</p>"},{"location":"package_rules/#debugging","title":"Debugging","text":"<p>You can verify the list of rules discovered and configured by <code>dbt-score</code> by running:</p> <pre><code>dbt-score list\ndbt-score list --namespace dbt_score_rules.dbtviz  # filter by a given namespace\n</code></pre> <p>If your custom rules are not present, try to open a Python shell and import them:</p> <pre><code>import dbt_score_rules.dbtviz\n</code></pre> <p>If it doesn't succeed, it means the rules are not properly installed or not within the Python path.</p>"},{"location":"programmatic_invocations/","title":"Programmatic invocations","text":"<p><code>dbt-score</code> can be used interactively as a CLI tool, but also be integrated in Continuous Integration systems and anywhere else it makes sense.</p>"},{"location":"programmatic_invocations/#machine-readable-output","title":"Machine-readable output","text":"<p>In order to programmatically use the output of <code>dbt-score</code> in another program, the JSON formatter can be used:</p> <pre><code>$ dbt-score lint --format json\n{\n  \"models\": {\n    \"model1\": {\n      \"score\": 8.666666666666668,\n      \"badge\": \"\ud83e\udd48\",\n      \"pass\": true,\n      \"results\": {\n        \"dbt_score.rules.generic.columns_have_description\": {\n          \"result\": \"OK\",\n          \"severity\": \"medium\",\n          \"message\": null\n        },\n        \"dbt_score.rules.generic.has_description\": {\n          \"result\": \"OK\",\n          \"severity\": \"medium\",\n          \"message\": null\n        },\n        \"dbt_score.rules.generic.has_owner\": {\n          \"result\": \"WARN\",\n          \"severity\": \"medium\",\n          \"message\": \"Model lacks an owner.\"\n        },\n        \"dbt_score.rules.generic.has_example_sql\": {\n          \"result\": \"OK\",\n          \"severity\": \"low\",\n          \"message\": null\n        },\n        \"dbt_score.rules.generic.sql_has_reasonable_number_of_lines\": {\n          \"result\": \"OK\",\n          \"severity\": \"medium\",\n          \"message\": null\n        }\n      }\n    }\n  },\n  \"project\": {\n    \"score\": 8.666666666666668,\n    \"badge\": \"\ud83e\udd48\",\n    \"pass\": true\n  }\n}\n</code></pre>"},{"location":"programmatic_invocations/#exit-codes","title":"Exit codes","text":"<p>When <code>dbt-score</code> terminates, it exists with one of the following exit codes:</p> <ul> <li><code>0</code> in case of successful termination. This is the happy case, when the   project being linted either doesn't raise any warning, or the warnings are   small enough to be above the thresholds. This generally means \"successful   linting\".</li> <li><code>1</code> in case of linting errors. This is the unhappy case: some entities in the   project raise enough warnings to have a score below the defined thresholds.   This generally means \"linting doesn't pass\".</li> <li><code>2</code> in case of an unexpected error. This happens for example if something is   misconfigured (for example a faulty dbt project), or the wrong parameters are   given to the CLI. This generally means \"setup needs to be fixed\".</li> </ul>"},{"location":"reference/cli/","title":"CLI","text":"<p>CLI interface.</p>"},{"location":"reference/cli/#dbt_score.cli.cli","title":"<code>cli()</code>","text":"<p>CLI entrypoint.</p> Source code in <code>src/dbt_score/cli.py</code> <pre><code>@click.version_option(message=\"%(version)s\")\n@click.group(\n    help=f\"\\b{BANNER}\",\n    invoke_without_command=False,\n    context_settings={\"help_option_names\": [\"-h\", \"--help\"]},\n)\ndef cli() -&gt; None:\n    \"\"\"CLI entrypoint.\"\"\"\n</code></pre>"},{"location":"reference/cli/#dbt_score.cli.lint","title":"<code>lint(ctx, format, select, namespace, disabled_rule, manifest, run_dbt_parse, fail_project_under, fail_any_item_under, show, debug)</code>","text":"<p>Lint dbt metadata.</p> Source code in <code>src/dbt_score/cli.py</code> <pre><code>@cli.command()\n@click.option(\n    \"--format\",\n    \"-f\",\n    help=\"Output format. Plain is suitable for terminals, manifest for rich \"\n    \"documentation, json for machine-readable output.\",\n    type=click.Choice([\"plain\", \"manifest\", \"ascii\", \"json\"]),\n    default=\"plain\",\n)\n@click.option(\n    \"--select\",\n    \"-s\",\n    help=\"Specify the nodes to include.\",\n    multiple=True,\n)\n@click.option(\n    \"--namespace\",\n    \"-n\",\n    help=\"Namespace to look for rules.\",\n    default=None,\n    multiple=True,\n)\n@click.option(\n    \"--disabled-rule\",\n    help=\"Rule to disable.\",\n    default=None,\n    multiple=True,\n)\n@click.option(\n    \"--manifest\",\n    \"-m\",\n    help=\"Manifest filepath.\",\n    type=click.Path(path_type=Path),\n    default=get_default_manifest_path(),\n)\n@click.option(\n    \"--run-dbt-parse\",\n    \"-p\",\n    help=\"Run dbt parse.\",\n    is_flag=True,\n    default=False,\n)\n@click.option(\n    \"--fail-project-under\",\n    help=\"Fail if the project score is under this value.\",\n    type=float,\n    is_flag=False,\n    default=None,\n)\n@click.option(\n    \"--fail-any-item-under\",\n    help=\"Fail if any evaluable item is under this value.\",\n    type=float,\n    is_flag=False,\n    default=None,\n)\n@click.option(\n    \"--show\",\n    help=\"Type of output which should be shown \"\n    \"when using `plain` as `--format`. \"\n    \"`all` shows all items and all rules. \"\n    \"`failing-items` shows failing rules of failing items. \"\n    \"`failing-rules` shows failing rules of all items. \"\n    \"Default is --failing-rules.\",\n    type=click.Choice([\"all\", \"failing-items\", \"failing-rules\"]),\n    is_flag=False,\n    default=\"failing-rules\",\n)\n@click.option(\n    \"--debug\",\n    \"-d\",\n    help=\"Jump in a debugger in case of rule failure to evaluate.\",\n    type=bool,\n    is_flag=True,\n    default=False,\n)\n@click.pass_context\ndef lint(  # noqa: PLR0912, PLR0913, C901\n    ctx: click.Context,\n    format: Literal[\"plain\", \"manifest\", \"ascii\"],\n    select: tuple[str],\n    namespace: list[str],\n    disabled_rule: list[str],\n    manifest: Path,\n    run_dbt_parse: bool,\n    fail_project_under: float,\n    fail_any_item_under: float,\n    show: Literal[\"all\", \"failing-items\", \"failing-rules\"],\n    debug: bool,\n) -&gt; None:\n    \"\"\"Lint dbt metadata.\"\"\"\n    manifest_provided = (\n        click.get_current_context().get_parameter_source(\"manifest\")\n        != ParameterSource.DEFAULT\n    )\n    if manifest_provided and run_dbt_parse:\n        raise click.UsageError(\"--run-dbt-parse cannot be used with --manifest.\")\n\n    config = Config()\n    config.load()\n    if namespace:\n        config.overload({\"rule_namespaces\": namespace})\n    if disabled_rule:\n        config.overload({\"disabled_rules\": disabled_rule})\n    if fail_project_under:\n        config.overload({\"fail_project_under\": fail_project_under})\n    if fail_any_item_under:\n        config.overload({\"fail_any_item_under\": fail_any_item_under})\n    if show:\n        config.overload({\"show\": show})\n    if debug:\n        config.overload({\"debug\": debug})\n\n    try:\n        if run_dbt_parse:\n            dbt_parse()\n        evaluation = lint_dbt_project(\n            manifest_path=manifest, config=config, format=format, select=select\n        )\n\n    except FileNotFoundError:\n        logger.error(\n            \"dbt's manifest.json could not be found. If you're in a dbt project, be \"\n            \"sure to run 'dbt parse' first, or use the option '--run-dbt-parse'.\"\n        )\n        ctx.exit(2)\n\n    except DbtParseException as exc:\n        logger.error(exc)\n        ctx.exit(2)\n\n    except Exception:\n        logger.error(traceback.format_exc())\n        ctx.exit(2)\n\n    if (\n        any(x.value &lt; config.fail_any_item_under for x in evaluation.scores.values())\n        or evaluation.project_score.value &lt; config.fail_project_under\n    ):\n        ctx.exit(1)\n</code></pre>"},{"location":"reference/cli/#dbt_score.cli.list_command","title":"<code>list_command(namespace, disabled_rule, title, format)</code>","text":"<p>Display rules list.</p> Source code in <code>src/dbt_score/cli.py</code> <pre><code>@cli.command(name=\"list\")\n@click.option(\n    \"--namespace\",\n    \"-n\",\n    help=\"Namespace.\",\n    default=None,\n    multiple=True,\n)\n@click.option(\n    \"--disabled-rule\",\n    help=\"Rule to disable.\",\n    default=None,\n    multiple=True,\n)\n@click.option(\n    \"--title\",\n    help=\"Page title (Markdown only).\",\n    default=None,\n)\n@click.option(\n    \"--format\",\n    \"-f\",\n    help=\"Output format.\",\n    type=click.Choice([\"terminal\", \"markdown\"]),\n    default=\"terminal\",\n)\ndef list_command(\n    namespace: list[str], disabled_rule: list[str], title: str, format: str\n) -&gt; None:\n    \"\"\"Display rules list.\"\"\"\n    config = Config()\n    config.load()\n    if namespace:\n        config.overload({\"rule_namespaces\": namespace})\n    if disabled_rule:\n        config.overload({\"disabled_rules\": disabled_rule})\n\n    display_catalog(config, title, format)\n</code></pre>"},{"location":"reference/config/","title":"Config","text":"<p>This module is responsible for loading configuration.</p>"},{"location":"reference/config/#dbt_score.config.Badge","title":"<code>Badge</code>  <code>dataclass</code>","text":"<p>Badge object.</p> Source code in <code>src/dbt_score/config.py</code> <pre><code>@dataclass\nclass Badge:\n    \"\"\"Badge object.\"\"\"\n\n    icon: str\n    threshold: float\n</code></pre>"},{"location":"reference/config/#dbt_score.config.BadgeConfig","title":"<code>BadgeConfig</code>  <code>dataclass</code>","text":"<p>Configuration for badges.</p> Source code in <code>src/dbt_score/config.py</code> <pre><code>@dataclass\nclass BadgeConfig:\n    \"\"\"Configuration for badges.\"\"\"\n\n    third: Badge = field(default_factory=lambda: Badge(\"\ud83e\udd49\", 6.0))\n    second: Badge = field(default_factory=lambda: Badge(\"\ud83e\udd48\", 8.0))\n    first: Badge = field(default_factory=lambda: Badge(\"\ud83e\udd47\", 10.0))\n    wip: Badge = field(default_factory=lambda: Badge(\"\ud83d\udea7\", 0.0))\n\n    def validate(self) -&gt; None:\n        \"\"\"Validate the badge configuration.\"\"\"\n        if not (self.first.threshold &gt; self.second.threshold &gt; self.third.threshold):\n            raise ValueError(\"Invalid badge thresholds.\")\n        if self.first.threshold &gt; 10.0:  # noqa: PLR2004 [magic-value-comparison]\n            raise ValueError(\"first threshold must 10.0 or lower.\")\n        if self.third.threshold &lt; 0.0:\n            raise ValueError(\"third threshold must be 0.0 or higher.\")\n        if self.wip.threshold != 0.0:\n            raise AttributeError(\"wip badge cannot have a threshold configuration.\")\n</code></pre>"},{"location":"reference/config/#dbt_score.config.BadgeConfig.validate","title":"<code>validate()</code>","text":"<p>Validate the badge configuration.</p> Source code in <code>src/dbt_score/config.py</code> <pre><code>def validate(self) -&gt; None:\n    \"\"\"Validate the badge configuration.\"\"\"\n    if not (self.first.threshold &gt; self.second.threshold &gt; self.third.threshold):\n        raise ValueError(\"Invalid badge thresholds.\")\n    if self.first.threshold &gt; 10.0:  # noqa: PLR2004 [magic-value-comparison]\n        raise ValueError(\"first threshold must 10.0 or lower.\")\n    if self.third.threshold &lt; 0.0:\n        raise ValueError(\"third threshold must be 0.0 or higher.\")\n    if self.wip.threshold != 0.0:\n        raise AttributeError(\"wip badge cannot have a threshold configuration.\")\n</code></pre>"},{"location":"reference/config/#dbt_score.config.Config","title":"<code>Config</code>","text":"<p>Configuration for dbt-score.</p> Source code in <code>src/dbt_score/config.py</code> <pre><code>class Config:\n    \"\"\"Configuration for dbt-score.\"\"\"\n\n    _main_section: Final[str] = \"tool.dbt-score\"\n    _options: Final[list[str]] = [\n        \"rule_namespaces\",\n        \"disabled_rules\",\n        \"inject_cwd_in_python_path\",\n        \"fail_project_under\",\n        \"fail_any_item_under\",\n        \"show\",\n        \"debug\",\n    ]\n    _rules_section: Final[str] = \"rules\"\n    _badges_section: Final[str] = \"badges\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize the Config object.\"\"\"\n        self.rule_namespaces: list[str] = [\"dbt_score.rules\", \"dbt_score_rules\"]\n        self.disabled_rules: list[str] = []\n        self.inject_cwd_in_python_path = True\n        self.rules_config: dict[str, RuleConfig] = {}\n        self.config_file: Path | None = None\n        self.badge_config: BadgeConfig = BadgeConfig()\n        self.fail_project_under: float = 5.0\n        self.fail_any_item_under: float = 5.0\n        self.show: str = \"failing-rules\"\n        self.debug: bool = False\n\n    def set_option(self, option: str, value: Any) -&gt; None:\n        \"\"\"Set an option in the config.\"\"\"\n        setattr(self, option, value)\n\n    def _load_toml_file(self, file: str) -&gt; None:\n        \"\"\"Load the options from a TOML file.\"\"\"\n        with open(file, \"rb\") as f:\n            toml_data = tomllib.load(f)\n\n        tools = toml_data.get(\"tool\", {})\n        dbt_score_config = tools.get(\"dbt-score\", {})\n        rules_config = dbt_score_config.pop(self._rules_section, {})\n        badge_config = dbt_score_config.pop(self._badges_section, {})\n\n        # Main configuration\n        for option, value in dbt_score_config.items():\n            if option in self._options:\n                self.set_option(option, value)\n            elif not isinstance(\n                value, dict\n            ):  # If value is a dictionary, it's another section\n                logger.warning(\n                    f\"Option {option} in {self._main_section} not supported.\"\n                )\n\n        # Badge configuration\n        for name, config in badge_config.items():\n            try:\n                default_config = getattr(self.badge_config, name)\n                updated_config = replace(default_config, **config)\n                setattr(self.badge_config, name, updated_config)\n            except AttributeError as e:\n                options = list(BadgeConfig.__annotations__.keys())\n                raise AttributeError(f\"Config only accepts badges: {options}.\") from e\n            except TypeError as e:\n                options = list(Badge.__annotations__.keys())\n                if name == \"wip\":\n                    options.remove(\"threshold\")\n                raise AttributeError(\n                    f\"Badge {name}: config only accepts {options}.\"\n                ) from e\n\n        self.badge_config.validate()\n\n        # Rule configuration\n        self.rules_config = {\n            name: RuleConfig.from_dict(config) for name, config in rules_config.items()\n        }\n\n    @staticmethod\n    def get_config_file(directory: Path) -&gt; Path | None:\n        \"\"\"Get the config file.\"\"\"\n        candidates = [directory]\n        candidates.extend(directory.parents)\n        for path in candidates:\n            config_file = path / DEFAULT_CONFIG_FILE\n            if config_file.exists():\n                return config_file\n\n    def load(self) -&gt; None:\n        \"\"\"Load the config.\"\"\"\n        config_file = self.get_config_file(Path.cwd())\n        if config_file:\n            self._load_toml_file(str(config_file))\n\n    def overload(self, values: dict[str, Any]) -&gt; None:\n        \"\"\"Overload config with additional values.\"\"\"\n        for key, value in values.items():\n            self.set_option(key, value)\n</code></pre>"},{"location":"reference/config/#dbt_score.config.Config.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the Config object.</p> Source code in <code>src/dbt_score/config.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the Config object.\"\"\"\n    self.rule_namespaces: list[str] = [\"dbt_score.rules\", \"dbt_score_rules\"]\n    self.disabled_rules: list[str] = []\n    self.inject_cwd_in_python_path = True\n    self.rules_config: dict[str, RuleConfig] = {}\n    self.config_file: Path | None = None\n    self.badge_config: BadgeConfig = BadgeConfig()\n    self.fail_project_under: float = 5.0\n    self.fail_any_item_under: float = 5.0\n    self.show: str = \"failing-rules\"\n    self.debug: bool = False\n</code></pre>"},{"location":"reference/config/#dbt_score.config.Config.get_config_file","title":"<code>get_config_file(directory)</code>  <code>staticmethod</code>","text":"<p>Get the config file.</p> Source code in <code>src/dbt_score/config.py</code> <pre><code>@staticmethod\ndef get_config_file(directory: Path) -&gt; Path | None:\n    \"\"\"Get the config file.\"\"\"\n    candidates = [directory]\n    candidates.extend(directory.parents)\n    for path in candidates:\n        config_file = path / DEFAULT_CONFIG_FILE\n        if config_file.exists():\n            return config_file\n</code></pre>"},{"location":"reference/config/#dbt_score.config.Config.load","title":"<code>load()</code>","text":"<p>Load the config.</p> Source code in <code>src/dbt_score/config.py</code> <pre><code>def load(self) -&gt; None:\n    \"\"\"Load the config.\"\"\"\n    config_file = self.get_config_file(Path.cwd())\n    if config_file:\n        self._load_toml_file(str(config_file))\n</code></pre>"},{"location":"reference/config/#dbt_score.config.Config.overload","title":"<code>overload(values)</code>","text":"<p>Overload config with additional values.</p> Source code in <code>src/dbt_score/config.py</code> <pre><code>def overload(self, values: dict[str, Any]) -&gt; None:\n    \"\"\"Overload config with additional values.\"\"\"\n    for key, value in values.items():\n        self.set_option(key, value)\n</code></pre>"},{"location":"reference/config/#dbt_score.config.Config.set_option","title":"<code>set_option(option, value)</code>","text":"<p>Set an option in the config.</p> Source code in <code>src/dbt_score/config.py</code> <pre><code>def set_option(self, option: str, value: Any) -&gt; None:\n    \"\"\"Set an option in the config.\"\"\"\n    setattr(self, option, value)\n</code></pre>"},{"location":"reference/dbt_utils/","title":"dbt utils","text":"<p>dbt utilities.</p>"},{"location":"reference/dbt_utils/#dbt_score.dbt_utils.DbtLsException","title":"<code>DbtLsException</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when dbt ls fails.</p> Source code in <code>src/dbt_score/dbt_utils.py</code> <pre><code>class DbtLsException(Exception):\n    \"\"\"Raised when dbt ls fails.\"\"\"\n</code></pre>"},{"location":"reference/dbt_utils/#dbt_score.dbt_utils.DbtNotInstalledException","title":"<code>DbtNotInstalledException</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when trying to run dbt when dbt is not installed.</p> Source code in <code>src/dbt_score/dbt_utils.py</code> <pre><code>class DbtNotInstalledException(Exception):\n    \"\"\"Raised when trying to run dbt when dbt is not installed.\"\"\"\n</code></pre>"},{"location":"reference/dbt_utils/#dbt_score.dbt_utils.DbtParseException","title":"<code>DbtParseException</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when dbt parse fails.</p> Source code in <code>src/dbt_score/dbt_utils.py</code> <pre><code>class DbtParseException(Exception):\n    \"\"\"Raised when dbt parse fails.\"\"\"\n\n    def __init__(self, root_cause: BaseException | None = None):\n        \"\"\"Initialize the exception.\"\"\"\n        super().__init__()\n        self.root_cause = root_cause\n\n    def __str__(self) -&gt; str:\n        \"\"\"Return a string representation of the exception.\"\"\"\n        if self.root_cause:\n            return f\"dbt parse failed.\\n\\n{self.root_cause!s}\"\n\n        return (\n            \"dbt parse failed. Root cause not found. Please run `dbt parse` manually.\"\n        )\n</code></pre>"},{"location":"reference/dbt_utils/#dbt_score.dbt_utils.DbtParseException.__init__","title":"<code>__init__(root_cause=None)</code>","text":"<p>Initialize the exception.</p> Source code in <code>src/dbt_score/dbt_utils.py</code> <pre><code>def __init__(self, root_cause: BaseException | None = None):\n    \"\"\"Initialize the exception.\"\"\"\n    super().__init__()\n    self.root_cause = root_cause\n</code></pre>"},{"location":"reference/dbt_utils/#dbt_score.dbt_utils.DbtParseException.__str__","title":"<code>__str__()</code>","text":"<p>Return a string representation of the exception.</p> Source code in <code>src/dbt_score/dbt_utils.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Return a string representation of the exception.\"\"\"\n    if self.root_cause:\n        return f\"dbt parse failed.\\n\\n{self.root_cause!s}\"\n\n    return (\n        \"dbt parse failed. Root cause not found. Please run `dbt parse` manually.\"\n    )\n</code></pre>"},{"location":"reference/dbt_utils/#dbt_score.dbt_utils.dbt_ls","title":"<code>dbt_ls(select)</code>","text":"<p>Run dbt ls.</p> Source code in <code>src/dbt_score/dbt_utils.py</code> <pre><code>@dbt_required\ndef dbt_ls(select: Iterable[str] | None) -&gt; Iterable[str]:\n    \"\"\"Run dbt ls.\"\"\"\n    cmd = [\n        \"ls\",\n        \"--resource-types\",\n        \"model\",\n        \"source\",\n        \"snapshot\",\n        \"exposure\",\n        \"seed\",\n        \"--output\",\n        \"name\",\n    ]\n    if select:\n        cmd += [\"--select\", *select]\n\n    with _disable_dbt_stdout():\n        result: \"dbtRunnerResult\" = dbtRunner().invoke(cmd)\n\n    if not result.success:\n        raise DbtLsException(\"dbt ls failed.\") from result.exception\n\n    selected = cast(Iterable[str], result.result)  # mypy hint\n    return selected\n</code></pre>"},{"location":"reference/dbt_utils/#dbt_score.dbt_utils.dbt_parse","title":"<code>dbt_parse()</code>","text":"<p>Parse a dbt project.</p> <p>Returns:</p> Type Description <code>dbtRunnerResult</code> <p>The dbt parse run result.</p> <p>Raises:</p> Type Description <code>DbtParseException</code> <p>dbt parse failed.</p> Source code in <code>src/dbt_score/dbt_utils.py</code> <pre><code>@dbt_required\ndef dbt_parse() -&gt; \"dbtRunnerResult\":\n    \"\"\"Parse a dbt project.\n\n    Returns:\n        The dbt parse run result.\n\n    Raises:\n        DbtParseException: dbt parse failed.\n    \"\"\"\n    with _disable_dbt_stdout():\n        result: \"dbtRunnerResult\" = dbtRunner().invoke([\"parse\"])\n\n    if not result.success:\n        raise DbtParseException(root_cause=result.exception)\n\n    return result\n</code></pre>"},{"location":"reference/dbt_utils/#dbt_score.dbt_utils.dbt_required","title":"<code>dbt_required(func)</code>","text":"<p>Decorator for methods that require dbt to be installed.</p> Source code in <code>src/dbt_score/dbt_utils.py</code> <pre><code>def dbt_required(func: Callable[..., Any]) -&gt; Callable[..., Any]:\n    \"\"\"Decorator for methods that require dbt to be installed.\"\"\"\n\n    @wraps(func)\n    def wrapper(*args: Any, **kwargs: Any) -&gt; Any:\n        if not DBT_INSTALLED:\n            raise DbtNotInstalledException(\n                \"This option requires dbt to be installed in the same Python\"\n                \"environment as dbt-score.\"\n            )\n        return func(*args, **kwargs)\n\n    return wrapper\n</code></pre>"},{"location":"reference/dbt_utils/#dbt_score.dbt_utils.get_default_manifest_path","title":"<code>get_default_manifest_path()</code>","text":"<p>Get the manifest path.</p> Source code in <code>src/dbt_score/dbt_utils.py</code> <pre><code>def get_default_manifest_path() -&gt; Path:\n    \"\"\"Get the manifest path.\"\"\"\n    return (\n        Path().cwd()\n        / os.getenv(\"DBT_PROJECT_DIR\", \"\")\n        / os.getenv(\"DBT_TARGET_DIR\", \"target\")\n        / \"manifest.json\"\n    )\n</code></pre>"},{"location":"reference/evaluation/","title":"Evaluation","text":"<p>This module is responsible for evaluating rules.</p>"},{"location":"reference/evaluation/#dbt_score.evaluation.Evaluation","title":"<code>Evaluation</code>","text":"<p>Evaluate a set of rules on a set of nodes.</p> Source code in <code>src/dbt_score/evaluation.py</code> <pre><code>class Evaluation:\n    \"\"\"Evaluate a set of rules on a set of nodes.\"\"\"\n\n    def __init__(\n        self,\n        rule_registry: RuleRegistry,\n        manifest_loader: ManifestLoader,\n        formatter: Formatter,\n        scorer: Scorer,\n        config: Config,\n    ) -&gt; None:\n        \"\"\"Create an Evaluation object.\n\n        Args:\n            rule_registry: A rule registry to access rules.\n            manifest_loader: A manifest loader to access dbt metadata.\n            formatter: A formatter to display results.\n            scorer: A scorer to compute scores.\n            config: A configuration.\n        \"\"\"\n        self._rule_registry = rule_registry\n        self._manifest_loader = manifest_loader\n        self._formatter = formatter\n        self._scorer = scorer\n        self._config = config\n\n        # For each evaluable, its results\n        self.results: dict[Evaluable, EvaluableResultsType] = {}\n\n        # For each evaluable, its computed score\n        self.scores: dict[Evaluable, Score] = {}\n\n        # The aggregated project score\n        self.project_score: Score\n\n    def evaluate(self) -&gt; None:\n        \"\"\"Evaluate all rules.\"\"\"\n        rules = self._rule_registry.rules.values()\n\n        for evaluable in chain(\n            self._manifest_loader.models.values(),\n            self._manifest_loader.sources.values(),\n            self._manifest_loader.snapshots.values(),\n            self._manifest_loader.exposures.values(),\n            self._manifest_loader.seeds.values(),\n        ):\n            # type inference on elements from `chain` is wonky\n            # and resolves to superclass HasColumnsMixin\n            evaluable = cast(Evaluable, evaluable)\n            self.results[evaluable] = {}\n            for rule in rules:\n                try:\n                    if rule.should_evaluate(evaluable):\n                        result = rule.evaluate(evaluable, **rule.config)\n                        self.results[evaluable][rule.__class__] = result\n                except Exception as e:\n                    if self._config.debug:\n                        traceback.print_exc()\n                        pdb.post_mortem()\n                    self.results[evaluable][rule.__class__] = e\n\n            self.scores[evaluable] = self._scorer.score_evaluable(\n                self.results[evaluable]\n            )\n            self._formatter.evaluable_evaluated(\n                evaluable, self.results[evaluable], self.scores[evaluable]\n            )\n\n        # Compute score for project\n        self.project_score = self._scorer.score_aggregate_evaluables(\n            list(self.scores.values())\n        )\n\n        # Add null check before calling project_evaluated\n        if (\n            self._manifest_loader.models\n            or self._manifest_loader.sources\n            or self._manifest_loader.snapshots\n            or self._manifest_loader.exposures\n            or self._manifest_loader.seeds\n        ):\n            self._formatter.project_evaluated(self.project_score)\n</code></pre>"},{"location":"reference/evaluation/#dbt_score.evaluation.Evaluation.__init__","title":"<code>__init__(rule_registry, manifest_loader, formatter, scorer, config)</code>","text":"<p>Create an Evaluation object.</p> <p>Parameters:</p> Name Type Description Default <code>rule_registry</code> <code>RuleRegistry</code> <p>A rule registry to access rules.</p> required <code>manifest_loader</code> <code>ManifestLoader</code> <p>A manifest loader to access dbt metadata.</p> required <code>formatter</code> <code>Formatter</code> <p>A formatter to display results.</p> required <code>scorer</code> <code>Scorer</code> <p>A scorer to compute scores.</p> required <code>config</code> <code>Config</code> <p>A configuration.</p> required Source code in <code>src/dbt_score/evaluation.py</code> <pre><code>def __init__(\n    self,\n    rule_registry: RuleRegistry,\n    manifest_loader: ManifestLoader,\n    formatter: Formatter,\n    scorer: Scorer,\n    config: Config,\n) -&gt; None:\n    \"\"\"Create an Evaluation object.\n\n    Args:\n        rule_registry: A rule registry to access rules.\n        manifest_loader: A manifest loader to access dbt metadata.\n        formatter: A formatter to display results.\n        scorer: A scorer to compute scores.\n        config: A configuration.\n    \"\"\"\n    self._rule_registry = rule_registry\n    self._manifest_loader = manifest_loader\n    self._formatter = formatter\n    self._scorer = scorer\n    self._config = config\n\n    # For each evaluable, its results\n    self.results: dict[Evaluable, EvaluableResultsType] = {}\n\n    # For each evaluable, its computed score\n    self.scores: dict[Evaluable, Score] = {}\n\n    # The aggregated project score\n    self.project_score: Score\n</code></pre>"},{"location":"reference/evaluation/#dbt_score.evaluation.Evaluation.evaluate","title":"<code>evaluate()</code>","text":"<p>Evaluate all rules.</p> Source code in <code>src/dbt_score/evaluation.py</code> <pre><code>def evaluate(self) -&gt; None:\n    \"\"\"Evaluate all rules.\"\"\"\n    rules = self._rule_registry.rules.values()\n\n    for evaluable in chain(\n        self._manifest_loader.models.values(),\n        self._manifest_loader.sources.values(),\n        self._manifest_loader.snapshots.values(),\n        self._manifest_loader.exposures.values(),\n        self._manifest_loader.seeds.values(),\n    ):\n        # type inference on elements from `chain` is wonky\n        # and resolves to superclass HasColumnsMixin\n        evaluable = cast(Evaluable, evaluable)\n        self.results[evaluable] = {}\n        for rule in rules:\n            try:\n                if rule.should_evaluate(evaluable):\n                    result = rule.evaluate(evaluable, **rule.config)\n                    self.results[evaluable][rule.__class__] = result\n            except Exception as e:\n                if self._config.debug:\n                    traceback.print_exc()\n                    pdb.post_mortem()\n                self.results[evaluable][rule.__class__] = e\n\n        self.scores[evaluable] = self._scorer.score_evaluable(\n            self.results[evaluable]\n        )\n        self._formatter.evaluable_evaluated(\n            evaluable, self.results[evaluable], self.scores[evaluable]\n        )\n\n    # Compute score for project\n    self.project_score = self._scorer.score_aggregate_evaluables(\n        list(self.scores.values())\n    )\n\n    # Add null check before calling project_evaluated\n    if (\n        self._manifest_loader.models\n        or self._manifest_loader.sources\n        or self._manifest_loader.snapshots\n        or self._manifest_loader.exposures\n        or self._manifest_loader.seeds\n    ):\n        self._formatter.project_evaluated(self.project_score)\n</code></pre>"},{"location":"reference/exceptions/","title":"Exceptions","text":"<p>dbt-score exceptions.</p>"},{"location":"reference/exceptions/#dbt_score.exceptions.DuplicatedRuleException","title":"<code>DuplicatedRuleException</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Two rules with the same name are defined.</p> Source code in <code>src/dbt_score/exceptions.py</code> <pre><code>class DuplicatedRuleException(Exception):\n    \"\"\"Two rules with the same name are defined.\"\"\"\n\n    def __init__(self, rule_name: str):\n        \"\"\"Instantiate exception.\"\"\"\n        super().__init__(\n            f\"Rule {rule_name} is defined twice. Rules must have unique names.\"\n        )\n</code></pre>"},{"location":"reference/exceptions/#dbt_score.exceptions.DuplicatedRuleException.__init__","title":"<code>__init__(rule_name)</code>","text":"<p>Instantiate exception.</p> Source code in <code>src/dbt_score/exceptions.py</code> <pre><code>def __init__(self, rule_name: str):\n    \"\"\"Instantiate exception.\"\"\"\n    super().__init__(\n        f\"Rule {rule_name} is defined twice. Rules must have unique names.\"\n    )\n</code></pre>"},{"location":"reference/models/","title":"Models","text":"<p>Objects related to loading the dbt manifest.</p>"},{"location":"reference/models/#dbt_score.models.Column","title":"<code>Column</code>  <code>dataclass</code>","text":"<p>Represents a column.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the column.</p> <code>description</code> <code>str</code> <p>The description of the column.</p> <code>data_type</code> <code>str | None</code> <p>The data type of the column.</p> <code>meta</code> <code>dict[str, Any]</code> <p>The metadata attached to the column.</p> <code>constraints</code> <code>list[Constraint]</code> <p>The list of constraints attached to the column.</p> <code>tags</code> <code>list[str]</code> <p>The list of tags attached to the column.</p> <code>tests</code> <code>list[Test]</code> <p>The list of tests attached to the column.</p> <code>_raw_values</code> <code>dict[str, Any]</code> <p>The raw values of the column as defined in the node.</p> <code>_raw_test_values</code> <code>list[dict[str, Any]]</code> <p>The raw test values of the column as defined in the node.</p> Source code in <code>src/dbt_score/models.py</code> <pre><code>@dataclass\nclass Column:\n    \"\"\"Represents a column.\n\n    Attributes:\n        name: The name of the column.\n        description: The description of the column.\n        data_type: The data type of the column.\n        meta: The metadata attached to the column.\n        constraints: The list of constraints attached to the column.\n        tags: The list of tags attached to the column.\n        tests: The list of tests attached to the column.\n        _raw_values: The raw values of the column as defined in the node.\n        _raw_test_values: The raw test values of the column as defined in the node.\n    \"\"\"\n\n    name: str\n    description: str\n    data_type: str | None = None\n    meta: dict[str, Any] = field(default_factory=dict)\n    constraints: list[Constraint] = field(default_factory=list)\n    tags: list[str] = field(default_factory=list)\n    tests: list[Test] = field(default_factory=list)\n    _raw_values: dict[str, Any] = field(default_factory=dict)\n    _raw_test_values: list[dict[str, Any]] = field(default_factory=list)\n\n    @classmethod\n    def from_node_values(\n        cls, values: dict[str, Any], test_values: list[dict[str, Any]]\n    ) -&gt; \"Column\":\n        \"\"\"Create a column object from raw values.\"\"\"\n        return cls(\n            name=values[\"name\"],\n            description=values[\"description\"],\n            data_type=values[\"data_type\"],\n            meta=values[\"meta\"],\n            constraints=[\n                Constraint.from_raw_values(constraint)\n                for constraint in values[\"constraints\"]\n            ],\n            tags=values[\"tags\"],\n            tests=[Test.from_node(test) for test in test_values],\n            _raw_values=values,\n            _raw_test_values=test_values,\n        )\n</code></pre>"},{"location":"reference/models/#dbt_score.models.Column.from_node_values","title":"<code>from_node_values(values, test_values)</code>  <code>classmethod</code>","text":"<p>Create a column object from raw values.</p> Source code in <code>src/dbt_score/models.py</code> <pre><code>@classmethod\ndef from_node_values(\n    cls, values: dict[str, Any], test_values: list[dict[str, Any]]\n) -&gt; \"Column\":\n    \"\"\"Create a column object from raw values.\"\"\"\n    return cls(\n        name=values[\"name\"],\n        description=values[\"description\"],\n        data_type=values[\"data_type\"],\n        meta=values[\"meta\"],\n        constraints=[\n            Constraint.from_raw_values(constraint)\n            for constraint in values[\"constraints\"]\n        ],\n        tags=values[\"tags\"],\n        tests=[Test.from_node(test) for test in test_values],\n        _raw_values=values,\n        _raw_test_values=test_values,\n    )\n</code></pre>"},{"location":"reference/models/#dbt_score.models.Constraint","title":"<code>Constraint</code>  <code>dataclass</code>","text":"<p>Constraint for a model or a column.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>str</code> <p>The type of the constraint, e.g. <code>foreign_key</code>.</p> <code>name</code> <code>str | None</code> <p>The name of the constraint.</p> <code>expression</code> <code>str | None</code> <p>The expression of the constraint, e.g. <code>schema.other_table</code>.</p> <code>columns</code> <code>list[str] | None</code> <p>The columns for the constraint (only for model-level constraints).</p> <code>_raw_values</code> <code>dict[str, Any]</code> <p>The raw values of the constraint in the manifest.</p> Source code in <code>src/dbt_score/models.py</code> <pre><code>@dataclass\nclass Constraint:\n    \"\"\"Constraint for a model or a column.\n\n    Attributes:\n        type: The type of the constraint, e.g. `foreign_key`.\n        name: The name of the constraint.\n        expression: The expression of the constraint, e.g. `schema.other_table`.\n        columns: The columns for the constraint (only for model-level constraints).\n        _raw_values: The raw values of the constraint in the manifest.\n    \"\"\"\n\n    type: str\n    name: str | None = None\n    expression: str | None = None\n    columns: list[str] | None = None\n    _raw_values: dict[str, Any] = field(default_factory=dict)\n\n    @classmethod\n    def from_raw_values(cls, raw_values: dict[str, Any]) -&gt; \"Constraint\":\n        \"\"\"Create a constraint object from a constraint node in the manifest.\"\"\"\n        return cls(\n            type=raw_values[\"type\"],\n            name=raw_values[\"name\"],\n            expression=raw_values[\"expression\"],\n            columns=raw_values.get(\"columns\"),\n            _raw_values=raw_values,\n        )\n</code></pre>"},{"location":"reference/models/#dbt_score.models.Constraint.from_raw_values","title":"<code>from_raw_values(raw_values)</code>  <code>classmethod</code>","text":"<p>Create a constraint object from a constraint node in the manifest.</p> Source code in <code>src/dbt_score/models.py</code> <pre><code>@classmethod\ndef from_raw_values(cls, raw_values: dict[str, Any]) -&gt; \"Constraint\":\n    \"\"\"Create a constraint object from a constraint node in the manifest.\"\"\"\n    return cls(\n        type=raw_values[\"type\"],\n        name=raw_values[\"name\"],\n        expression=raw_values[\"expression\"],\n        columns=raw_values.get(\"columns\"),\n        _raw_values=raw_values,\n    )\n</code></pre>"},{"location":"reference/models/#dbt_score.models.Duration","title":"<code>Duration</code>  <code>dataclass</code>","text":"<p>Represents a duration used in SourceFreshness.</p> <p>This is referred to as <code>Time</code> in the dbt JSONSchema.</p> <p>Attributes:</p> Name Type Description <code>count</code> <code>int | None</code> <p>a positive integer</p> <code>period</code> <code>Literal['minute', 'hour', 'day'] | None</code> <p>\"minute\" | \"hour\" | \"day\"</p> Source code in <code>src/dbt_score/models.py</code> <pre><code>@dataclass\nclass Duration:\n    \"\"\"Represents a duration used in SourceFreshness.\n\n    This is referred to as `Time` in the dbt JSONSchema.\n\n    Attributes:\n        count: a positive integer\n        period: \"minute\" | \"hour\" | \"day\"\n    \"\"\"\n\n    count: int | None = None\n    period: Literal[\"minute\", \"hour\", \"day\"] | None = None\n</code></pre>"},{"location":"reference/models/#dbt_score.models.Exposure","title":"<code>Exposure</code>  <code>dataclass</code>","text":"<p>Represents a dbt exposure.</p> <p>Attributes:</p> Name Type Description <code>unique_id</code> <code>str</code> <p>The unique id of the exposure (e.g. <code>exposure.package.exposure1</code>).</p> <code>name</code> <code>str</code> <p>The name of the exposure.</p> <code>description</code> <code>str</code> <p>The description of the exposure.</p> <code>label</code> <code>str</code> <p>The label of the exposure.</p> <code>url</code> <code>str</code> <p>The url of the exposure.</p> <code>maturity</code> <code>str</code> <p>The maturity of the exposure.</p> <code>original_file_path</code> <code>str</code> <p>The path to the exposure file (e.g. <code>models/exposures/exposures.yml</code>).</p> <code>type</code> <code>str</code> <p>The type of the exposure, e.g. <code>application</code>.</p> <code>owner</code> <code>dict[str, Any]</code> <p>The owner of the exposure, e.g. <code>{\"name\": \"owner\", \"email\": \"owner@email.com\"}</code>.</p> <code>config</code> <code>dict[str, Any]</code> <p>The config of the exposure.</p> <code>meta</code> <code>dict[str, Any]</code> <p>The meta of the exposure.</p> <code>tags</code> <code>list[str]</code> <p>The list of tags attached to the exposure.</p> <code>depends_on</code> <code>dict[str, list[str]]</code> <p>The depends_on of the exposure.</p> <code>parents</code> <code>list[ParentType]</code> <p>The list of models, sources, and snapshot this exposure depends on.</p> <code>_raw_values</code> <code>dict[str, Any]</code> <p>The raw values of the exposure in the manifest.</p> Source code in <code>src/dbt_score/models.py</code> <pre><code>@dataclass\nclass Exposure:\n    \"\"\"Represents a dbt exposure.\n\n    Attributes:\n        unique_id: The unique id of the exposure (e.g. `exposure.package.exposure1`).\n        name: The name of the exposure.\n        description: The description of the exposure.\n        label: The label of the exposure.\n        url: The url of the exposure.\n        maturity: The maturity of the exposure.\n        original_file_path: The path to the exposure file\n            (e.g. `models/exposures/exposures.yml`).\n        type: The type of the exposure, e.g. `application`.\n        owner: The owner of the exposure,\n            e.g. `{\"name\": \"owner\", \"email\": \"owner@email.com\"}`.\n        config: The config of the exposure.\n        meta: The meta of the exposure.\n        tags: The list of tags attached to the exposure.\n        depends_on: The depends_on of the exposure.\n        parents: The list of models, sources, and snapshot this exposure depends on.\n        _raw_values: The raw values of the exposure in the manifest.\n    \"\"\"\n\n    unique_id: str\n    name: str\n    description: str\n    label: str\n    url: str\n    maturity: str\n    original_file_path: str\n    type: str\n    owner: dict[str, Any]\n    config: dict[str, Any]\n    meta: dict[str, Any]\n    tags: list[str]\n    depends_on: dict[str, list[str]] = field(default_factory=dict)\n    parents: list[ParentType] = field(default_factory=list)\n    _raw_values: dict[str, Any] = field(default_factory=dict)\n\n    @classmethod\n    def from_node(cls, node_values: dict[str, Any]) -&gt; \"Exposure\":\n        \"\"\"Create an exposure object from a node in the manifest.\"\"\"\n        return cls(\n            unique_id=node_values[\"unique_id\"],\n            name=node_values[\"name\"],\n            description=node_values[\"description\"],\n            label=node_values[\"label\"],\n            url=node_values[\"url\"],\n            maturity=node_values[\"maturity\"],\n            original_file_path=node_values[\"original_file_path\"],\n            type=node_values[\"type\"],\n            owner=node_values[\"owner\"],\n            config=node_values[\"config\"],\n            meta=node_values[\"meta\"],\n            tags=node_values[\"tags\"],\n            depends_on=node_values[\"depends_on\"],\n            _raw_values=node_values,\n        )\n\n    def __hash__(self) -&gt; int:\n        \"\"\"Compute a unique hash for an exposure.\"\"\"\n        return hash(self.unique_id)\n</code></pre>"},{"location":"reference/models/#dbt_score.models.Exposure.__hash__","title":"<code>__hash__()</code>","text":"<p>Compute a unique hash for an exposure.</p> Source code in <code>src/dbt_score/models.py</code> <pre><code>def __hash__(self) -&gt; int:\n    \"\"\"Compute a unique hash for an exposure.\"\"\"\n    return hash(self.unique_id)\n</code></pre>"},{"location":"reference/models/#dbt_score.models.Exposure.from_node","title":"<code>from_node(node_values)</code>  <code>classmethod</code>","text":"<p>Create an exposure object from a node in the manifest.</p> Source code in <code>src/dbt_score/models.py</code> <pre><code>@classmethod\ndef from_node(cls, node_values: dict[str, Any]) -&gt; \"Exposure\":\n    \"\"\"Create an exposure object from a node in the manifest.\"\"\"\n    return cls(\n        unique_id=node_values[\"unique_id\"],\n        name=node_values[\"name\"],\n        description=node_values[\"description\"],\n        label=node_values[\"label\"],\n        url=node_values[\"url\"],\n        maturity=node_values[\"maturity\"],\n        original_file_path=node_values[\"original_file_path\"],\n        type=node_values[\"type\"],\n        owner=node_values[\"owner\"],\n        config=node_values[\"config\"],\n        meta=node_values[\"meta\"],\n        tags=node_values[\"tags\"],\n        depends_on=node_values[\"depends_on\"],\n        _raw_values=node_values,\n    )\n</code></pre>"},{"location":"reference/models/#dbt_score.models.HasColumnsMixin","title":"<code>HasColumnsMixin</code>","text":"<p>Common methods for resource types that have columns.</p> Source code in <code>src/dbt_score/models.py</code> <pre><code>class HasColumnsMixin:\n    \"\"\"Common methods for resource types that have columns.\"\"\"\n\n    columns: list[Column]\n\n    def get_column(self, column_name: str) -&gt; Column | None:\n        \"\"\"Get a column by name.\"\"\"\n        for column in self.columns:\n            if column.name == column_name:\n                return column\n\n        return None\n\n    @staticmethod\n    def _get_columns(\n        node_values: dict[str, Any], test_values: list[dict[str, Any]]\n    ) -&gt; list[Column]:\n        \"\"\"Get columns from a node and its tests in the manifest.\"\"\"\n        return [\n            Column.from_node_values(\n                values,\n                [\n                    test\n                    for test in test_values\n                    if test.get(\"test_metadata\", {})\n                    .get(\"kwargs\", {})\n                    .get(\"column_name\", \"\")\n                    .strip(\"`\")  # BigQuery connector when \"quote: true\"\n                    == name\n                ],\n            )\n            for name, values in node_values.get(\"columns\", {}).items()\n        ]\n</code></pre>"},{"location":"reference/models/#dbt_score.models.HasColumnsMixin.get_column","title":"<code>get_column(column_name)</code>","text":"<p>Get a column by name.</p> Source code in <code>src/dbt_score/models.py</code> <pre><code>def get_column(self, column_name: str) -&gt; Column | None:\n    \"\"\"Get a column by name.\"\"\"\n    for column in self.columns:\n        if column.name == column_name:\n            return column\n\n    return None\n</code></pre>"},{"location":"reference/models/#dbt_score.models.ManifestLoader","title":"<code>ManifestLoader</code>","text":"<p>Load the evaluables from the manifest.</p> Source code in <code>src/dbt_score/models.py</code> <pre><code>class ManifestLoader:\n    \"\"\"Load the evaluables from the manifest.\"\"\"\n\n    def __init__(self, file_path: Path, select: Iterable[str] | None = None):\n        \"\"\"Initialize the ManifestLoader.\n\n        Args:\n            file_path: The file path of the JSON manifest.\n            select: An optional dbt selection.\n        \"\"\"\n        self.raw_manifest = json.loads(file_path.read_text(encoding=\"utf-8\"))\n        self.project_name = self.raw_manifest[\"metadata\"][\"project_name\"]\n        self.raw_nodes = {\n            node_id: node_values\n            for node_id, node_values in self.raw_manifest.get(\"nodes\", {}).items()\n            if node_values[\"package_name\"] == self.project_name\n        }\n        self.raw_sources = {\n            source_id: source_values\n            for source_id, source_values in self.raw_manifest.get(\"sources\", {}).items()\n            if source_values[\"package_name\"] == self.project_name\n        }\n        self.raw_exposures = {\n            exposure_id: exposure_values\n            for exposure_id, exposure_values in self.raw_manifest.get(\n                \"exposures\", {}\n            ).items()\n            if exposure_values[\"package_name\"] == self.project_name\n        }\n\n        self.models: dict[str, Model] = {}\n        self.tests: dict[str, list[dict[str, Any]]] = defaultdict(list)\n        self.sources: dict[str, Source] = {}\n        self.snapshots: dict[str, Snapshot] = {}\n        self.exposures: dict[str, Exposure] = {}\n        self.seeds: dict[str, Seed] = {}\n\n        self._reindex_tests()\n        self._load_models()\n        self._load_sources()\n        self._load_snapshots()\n        self._load_exposures()\n        self._load_seeds()\n        self._populate_relatives()\n\n        if select:\n            self._filter_evaluables(select)\n\n        if (\n            len(self.models)\n            + len(self.sources)\n            + len(self.snapshots)\n            + len(self.seeds)\n            + len(self.exposures)\n        ) == 0:\n            logger.warning(\"Nothing to evaluate!\")\n\n    def _load_models(self) -&gt; None:\n        \"\"\"Load the models from the manifest.\"\"\"\n        for node_id, node_values in self.raw_nodes.items():\n            if node_values.get(\"resource_type\") == \"model\":\n                model = Model.from_node(node_values, self.tests.get(node_id, []))\n                self.models[node_id] = model\n\n    def _load_sources(self) -&gt; None:\n        \"\"\"Load the sources from the manifest.\"\"\"\n        for source_id, source_values in self.raw_sources.items():\n            if source_values.get(\"resource_type\") == \"source\":\n                source = Source.from_node(source_values, self.tests.get(source_id, []))\n                self.sources[source_id] = source\n\n    def _load_snapshots(self) -&gt; None:\n        \"\"\"Load the snapshots from the manifest.\"\"\"\n        for node_id, node_values in self.raw_nodes.items():\n            if node_values.get(\"resource_type\") == \"snapshot\":\n                snapshot = Snapshot.from_node(node_values, self.tests.get(node_id, []))\n                self.snapshots[node_id] = snapshot\n\n    def _load_exposures(self) -&gt; None:\n        \"\"\"Load the exposures from the manifest.\"\"\"\n        for node_id, node_values in self.raw_exposures.items():\n            if node_values.get(\"resource_type\") == \"exposure\":\n                exposure = Exposure.from_node(node_values)\n                self.exposures[node_id] = exposure\n\n    def _load_seeds(self) -&gt; None:\n        \"\"\"Load the seeds from the manifest.\"\"\"\n        for node_id, node_values in self.raw_nodes.items():\n            if node_values.get(\"resource_type\") == \"seed\":\n                seed = Seed.from_node(node_values, self.tests.get(node_id, []))\n                self.seeds[node_id] = seed\n\n    def _reindex_tests(self) -&gt; None:\n        \"\"\"Index tests based on their associated evaluable.\"\"\"\n        for node_values in self.raw_nodes.values():\n            if node_values.get(\"resource_type\") == \"test\":\n                # Tests for models have a non-null value for `attached_node`\n                if attached_node := node_values.get(\"attached_node\"):\n                    self.tests[attached_node].append(node_values)\n\n                # Tests for sources or separate tests will have `attached_node` == null.\n                # They need to be attributed to the node id\n                # based on the `depends_on` field.\n                elif node_unique_id := next(\n                    iter(node_values.get(\"depends_on\", {}).get(\"nodes\", [])), None\n                ):\n                    self.tests[node_unique_id].append(node_values)\n\n    def _populate_relatives(self) -&gt; None:\n        \"\"\"Populate `parents` and `children` for all evaluables.\"\"\"\n        for node in (\n            list(self.models.values())\n            + list(self.snapshots.values())\n            + list(self.exposures.values())\n        ):\n            for parent_id in node.depends_on.get(\"nodes\", []):\n                if parent_id in self.models:\n                    node.parents.append(self.models[parent_id])\n                    self.models[parent_id].children.append(node)\n                elif parent_id in self.snapshots:\n                    node.parents.append(self.snapshots[parent_id])\n                    self.snapshots[parent_id].children.append(node)\n                elif parent_id in self.sources:\n                    node.parents.append(self.sources[parent_id])\n                    self.sources[parent_id].children.append(node)\n                elif parent_id in self.seeds:\n                    node.parents.append(self.seeds[parent_id])\n                    self.seeds[parent_id].children.append(node)\n\n    def _filter_evaluables(self, select: Iterable[str]) -&gt; None:\n        \"\"\"Filter evaluables like dbt's --select.\"\"\"\n        single_model_select = re.compile(r\"[a-zA-Z0-9_]+\")\n\n        if all(single_model_select.fullmatch(x) for x in select):\n            # Using '--select my_model' is a common case, which can easily be sped up by\n            # not invoking dbt\n            selected = select\n        else:\n            # Use dbt's implementation of --select\n            selected = dbt_ls(select)\n\n        self.models = {k: m for k, m in self.models.items() if m.name in selected}\n        self.sources = {\n            k: s for k, s in self.sources.items() if s.selector_name in selected\n        }\n        self.snapshots = {k: s for k, s in self.snapshots.items() if s.name in selected}\n        self.exposures = {k: e for k, e in self.exposures.items() if e.name in selected}\n        self.seeds = {k: s for k, s in self.seeds.items() if s.name in selected}\n</code></pre>"},{"location":"reference/models/#dbt_score.models.ManifestLoader.__init__","title":"<code>__init__(file_path, select=None)</code>","text":"<p>Initialize the ManifestLoader.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path</code> <p>The file path of the JSON manifest.</p> required <code>select</code> <code>Iterable[str] | None</code> <p>An optional dbt selection.</p> <code>None</code> Source code in <code>src/dbt_score/models.py</code> <pre><code>def __init__(self, file_path: Path, select: Iterable[str] | None = None):\n    \"\"\"Initialize the ManifestLoader.\n\n    Args:\n        file_path: The file path of the JSON manifest.\n        select: An optional dbt selection.\n    \"\"\"\n    self.raw_manifest = json.loads(file_path.read_text(encoding=\"utf-8\"))\n    self.project_name = self.raw_manifest[\"metadata\"][\"project_name\"]\n    self.raw_nodes = {\n        node_id: node_values\n        for node_id, node_values in self.raw_manifest.get(\"nodes\", {}).items()\n        if node_values[\"package_name\"] == self.project_name\n    }\n    self.raw_sources = {\n        source_id: source_values\n        for source_id, source_values in self.raw_manifest.get(\"sources\", {}).items()\n        if source_values[\"package_name\"] == self.project_name\n    }\n    self.raw_exposures = {\n        exposure_id: exposure_values\n        for exposure_id, exposure_values in self.raw_manifest.get(\n            \"exposures\", {}\n        ).items()\n        if exposure_values[\"package_name\"] == self.project_name\n    }\n\n    self.models: dict[str, Model] = {}\n    self.tests: dict[str, list[dict[str, Any]]] = defaultdict(list)\n    self.sources: dict[str, Source] = {}\n    self.snapshots: dict[str, Snapshot] = {}\n    self.exposures: dict[str, Exposure] = {}\n    self.seeds: dict[str, Seed] = {}\n\n    self._reindex_tests()\n    self._load_models()\n    self._load_sources()\n    self._load_snapshots()\n    self._load_exposures()\n    self._load_seeds()\n    self._populate_relatives()\n\n    if select:\n        self._filter_evaluables(select)\n\n    if (\n        len(self.models)\n        + len(self.sources)\n        + len(self.snapshots)\n        + len(self.seeds)\n        + len(self.exposures)\n    ) == 0:\n        logger.warning(\"Nothing to evaluate!\")\n</code></pre>"},{"location":"reference/models/#dbt_score.models.Model","title":"<code>Model</code>  <code>dataclass</code>","text":"<p>               Bases: <code>HasColumnsMixin</code></p> <p>Represents a dbt model.</p> <p>Attributes:</p> Name Type Description <code>unique_id</code> <code>str</code> <p>The id of the model, e.g. <code>model.package.model_name</code>.</p> <code>name</code> <code>str</code> <p>The name of the model.</p> <code>relation_name</code> <code>str</code> <p>The relation name of the model, e.g. <code>db.schema.model_name</code>.</p> <code>description</code> <code>str</code> <p>The full description of the model.</p> <code>original_file_path</code> <code>str</code> <p>The sql path of the model, <code>e.g. model_dir/dir/file.sql</code>.</p> <code>config</code> <code>dict[str, Any]</code> <p>The config of the model.</p> <code>meta</code> <code>dict[str, Any]</code> <p>The meta of the model.</p> <code>columns</code> <code>list[Column]</code> <p>The list of columns of the model.</p> <code>package_name</code> <code>str</code> <p>The package name of the model.</p> <code>database</code> <code>str</code> <p>The database name of the model.</p> <code>schema</code> <code>str</code> <p>The schema name of the model.</p> <code>raw_code</code> <code>str</code> <p>The raw code of the model.</p> <code>language</code> <code>str</code> <p>The language of the model, e.g. sql.</p> <code>access</code> <code>str</code> <p>The access level of the model, e.g. public.</p> <code>group</code> <code>str</code> <p>The group the model is in.</p> <code>alias</code> <code>str | None</code> <p>The alias of the model.</p> <code>patch_path</code> <code>str | None</code> <p>The yml path of the model, e.g. <code>package://model_dir/dir/file.yml</code>.</p> <code>tags</code> <code>list[str]</code> <p>The list of tags attached to the model.</p> <code>tests</code> <code>list[Test]</code> <p>The list of tests attached to the model.</p> <code>depends_on</code> <code>dict[str, list[str]]</code> <p>Dictionary of models/sources/macros that the model depends on.</p> <code>parents</code> <code>list[ParentType]</code> <p>The list of models, sources, and snapshots this model depends on.</p> <code>children</code> <code>list[ChildType]</code> <p>The list of models and snapshots that depend on this model.</p> <code>_raw_values</code> <code>dict[str, Any]</code> <p>The raw values of the model (node) in the manifest.</p> <code>_raw_test_values</code> <code>list[dict[str, Any]]</code> <p>The raw test values of the model (node) in the manifest.</p> Source code in <code>src/dbt_score/models.py</code> <pre><code>@dataclass\nclass Model(HasColumnsMixin):\n    \"\"\"Represents a dbt model.\n\n    Attributes:\n        unique_id: The id of the model, e.g. `model.package.model_name`.\n        name: The name of the model.\n        relation_name: The relation name of the model, e.g. `db.schema.model_name`.\n        description: The full description of the model.\n        original_file_path: The sql path of the model, `e.g. model_dir/dir/file.sql`.\n        config: The config of the model.\n        meta: The meta of the model.\n        columns: The list of columns of the model.\n        package_name: The package name of the model.\n        database: The database name of the model.\n        schema: The schema name of the model.\n        raw_code: The raw code of the model.\n        language: The language of the model, e.g. sql.\n        access: The access level of the model, e.g. public.\n        group: The group the model is in.\n        alias: The alias of the model.\n        patch_path: The yml path of the model, e.g. `package://model_dir/dir/file.yml`.\n        tags: The list of tags attached to the model.\n        tests: The list of tests attached to the model.\n        depends_on: Dictionary of models/sources/macros that the model depends on.\n        parents: The list of models, sources, and snapshots this model depends on.\n        children: The list of models and snapshots that depend on this model.\n        _raw_values: The raw values of the model (node) in the manifest.\n        _raw_test_values: The raw test values of the model (node) in the manifest.\n    \"\"\"\n\n    unique_id: str\n    name: str\n    relation_name: str\n    description: str\n    original_file_path: str\n    config: dict[str, Any]\n    meta: dict[str, Any]\n    columns: list[Column]\n    package_name: str\n    database: str\n    schema: str\n    raw_code: str\n    language: str\n    access: str\n    group: str\n    alias: str | None = None\n    patch_path: str | None = None\n    tags: list[str] = field(default_factory=list)\n    tests: list[Test] = field(default_factory=list)\n    depends_on: dict[str, list[str]] = field(default_factory=dict)\n    constraints: list[Constraint] = field(default_factory=list)\n    parents: list[ParentType] = field(default_factory=list)\n    children: list[ChildType] = field(default_factory=list)\n    _raw_values: dict[str, Any] = field(default_factory=dict)\n    _raw_test_values: list[dict[str, Any]] = field(default_factory=list)\n\n    @classmethod\n    def from_node(\n        cls, node_values: dict[str, Any], test_values: list[dict[str, Any]]\n    ) -&gt; \"Model\":\n        \"\"\"Create a model object from a node and it's tests in the manifest.\"\"\"\n        return cls(\n            unique_id=node_values[\"unique_id\"],\n            name=node_values[\"name\"],\n            relation_name=node_values[\"relation_name\"],\n            description=node_values[\"description\"],\n            original_file_path=node_values[\"original_file_path\"],\n            config=node_values[\"config\"],\n            meta=node_values[\"meta\"],\n            columns=cls._get_columns(node_values, test_values),\n            package_name=node_values[\"package_name\"],\n            database=node_values[\"database\"],\n            schema=node_values[\"schema\"],\n            raw_code=node_values[\"raw_code\"],\n            language=node_values[\"language\"],\n            access=node_values[\"access\"],\n            group=node_values[\"group\"],\n            alias=node_values[\"alias\"],\n            patch_path=node_values[\"patch_path\"],\n            tags=node_values[\"tags\"],\n            tests=[\n                Test.from_node(test)\n                for test in test_values\n                if not test.get(\"test_metadata\", {})\n                .get(\"kwargs\", {})\n                .get(\"column_name\")\n            ],\n            depends_on=node_values[\"depends_on\"],\n            constraints=[\n                Constraint.from_raw_values(constraint)\n                for constraint in node_values[\"constraints\"]\n            ],\n            parents=[],  # Will be populated later\n            _raw_values=node_values,\n            _raw_test_values=test_values,\n        )\n\n    def __hash__(self) -&gt; int:\n        \"\"\"Compute a unique hash for a model.\"\"\"\n        return hash(self.unique_id)\n</code></pre>"},{"location":"reference/models/#dbt_score.models.Model.__hash__","title":"<code>__hash__()</code>","text":"<p>Compute a unique hash for a model.</p> Source code in <code>src/dbt_score/models.py</code> <pre><code>def __hash__(self) -&gt; int:\n    \"\"\"Compute a unique hash for a model.\"\"\"\n    return hash(self.unique_id)\n</code></pre>"},{"location":"reference/models/#dbt_score.models.Model.from_node","title":"<code>from_node(node_values, test_values)</code>  <code>classmethod</code>","text":"<p>Create a model object from a node and it's tests in the manifest.</p> Source code in <code>src/dbt_score/models.py</code> <pre><code>@classmethod\ndef from_node(\n    cls, node_values: dict[str, Any], test_values: list[dict[str, Any]]\n) -&gt; \"Model\":\n    \"\"\"Create a model object from a node and it's tests in the manifest.\"\"\"\n    return cls(\n        unique_id=node_values[\"unique_id\"],\n        name=node_values[\"name\"],\n        relation_name=node_values[\"relation_name\"],\n        description=node_values[\"description\"],\n        original_file_path=node_values[\"original_file_path\"],\n        config=node_values[\"config\"],\n        meta=node_values[\"meta\"],\n        columns=cls._get_columns(node_values, test_values),\n        package_name=node_values[\"package_name\"],\n        database=node_values[\"database\"],\n        schema=node_values[\"schema\"],\n        raw_code=node_values[\"raw_code\"],\n        language=node_values[\"language\"],\n        access=node_values[\"access\"],\n        group=node_values[\"group\"],\n        alias=node_values[\"alias\"],\n        patch_path=node_values[\"patch_path\"],\n        tags=node_values[\"tags\"],\n        tests=[\n            Test.from_node(test)\n            for test in test_values\n            if not test.get(\"test_metadata\", {})\n            .get(\"kwargs\", {})\n            .get(\"column_name\")\n        ],\n        depends_on=node_values[\"depends_on\"],\n        constraints=[\n            Constraint.from_raw_values(constraint)\n            for constraint in node_values[\"constraints\"]\n        ],\n        parents=[],  # Will be populated later\n        _raw_values=node_values,\n        _raw_test_values=test_values,\n    )\n</code></pre>"},{"location":"reference/models/#dbt_score.models.Seed","title":"<code>Seed</code>  <code>dataclass</code>","text":"<p>               Bases: <code>HasColumnsMixin</code></p> <p>Represents a dbt seed.</p> <p>Attributes:</p> Name Type Description <code>unique_id</code> <code>str</code> <p>The id of the seed, e.g. <code>seed.package.seed_name</code>.</p> <code>name</code> <code>str</code> <p>The name of the seed.</p> <code>relation_name</code> <code>str</code> <p>The relation name of the seed, e.g. <code>db.schema.seed_name</code>.</p> <code>description</code> <code>str</code> <p>The full description of the seed.</p> <code>original_file_path</code> <code>str</code> <p>The seed path, e.g. <code>data/seed_name.csv</code>.</p> <code>config</code> <code>dict[str, Any]</code> <p>The config of the seed.</p> <code>meta</code> <code>dict[str, Any]</code> <p>The meta of the seed.</p> <code>columns</code> <code>list[Column]</code> <p>The list of columns of the seed.</p> <code>package_name</code> <code>str</code> <p>The package name of the seed.</p> <code>database</code> <code>str</code> <p>The database name of the seed.</p> <code>schema</code> <code>str</code> <p>The schema name of the seed.</p> <code>alias</code> <code>str | None</code> <p>The alias of the seed.</p> <code>patch_path</code> <code>str | None</code> <p>The yml path of the seed, e.g. <code>seeds.yml</code>.</p> <code>tags</code> <code>list[str]</code> <p>The list of tags attached to the seed.</p> <code>tests</code> <code>list[Test]</code> <p>The list of tests attached to the seed.</p> <code>children</code> <code>list[ChildType]</code> <p>The list of models and snapshots that depend on this seed.</p> <code>_raw_values</code> <code>dict[str, Any]</code> <p>The raw values of the seed (node) in the manifest.</p> <code>_raw_test_values</code> <code>list[dict[str, Any]]</code> <p>The raw test values of the seed (node) in the manifest.</p> Source code in <code>src/dbt_score/models.py</code> <pre><code>@dataclass\nclass Seed(HasColumnsMixin):\n    \"\"\"Represents a dbt seed.\n\n    Attributes:\n        unique_id: The id of the seed, e.g. `seed.package.seed_name`.\n        name: The name of the seed.\n        relation_name: The relation name of the seed, e.g. `db.schema.seed_name`.\n        description: The full description of the seed.\n        original_file_path: The seed path, e.g. `data/seed_name.csv`.\n        config: The config of the seed.\n        meta: The meta of the seed.\n        columns: The list of columns of the seed.\n        package_name: The package name of the seed.\n        database: The database name of the seed.\n        schema: The schema name of the seed.\n        alias: The alias of the seed.\n        patch_path: The yml path of the seed, e.g. `seeds.yml`.\n        tags: The list of tags attached to the seed.\n        tests: The list of tests attached to the seed.\n        children: The list of models and snapshots that depend on this seed.\n        _raw_values: The raw values of the seed (node) in the manifest.\n        _raw_test_values: The raw test values of the seed (node) in the manifest.\n    \"\"\"\n\n    unique_id: str\n    name: str\n    relation_name: str\n    description: str\n    original_file_path: str\n    config: dict[str, Any]\n    meta: dict[str, Any]\n    columns: list[Column]\n    package_name: str\n    database: str\n    schema: str\n    alias: str | None = None\n    patch_path: str | None = None\n    tags: list[str] = field(default_factory=list)\n    tests: list[Test] = field(default_factory=list)\n    children: list[ChildType] = field(default_factory=list)\n    _raw_values: dict[str, Any] = field(default_factory=dict)\n    _raw_test_values: list[dict[str, Any]] = field(default_factory=list)\n\n    @classmethod\n    def from_node(\n        cls, node_values: dict[str, Any], test_values: list[dict[str, Any]]\n    ) -&gt; \"Seed\":\n        \"\"\"Create a seed object from a node and its tests in the manifest.\"\"\"\n        return cls(\n            unique_id=node_values[\"unique_id\"],\n            name=node_values[\"name\"],\n            relation_name=node_values[\"relation_name\"],\n            description=node_values[\"description\"],\n            original_file_path=node_values[\"original_file_path\"],\n            config=node_values[\"config\"],\n            meta=node_values[\"meta\"],\n            columns=cls._get_columns(node_values, test_values),\n            package_name=node_values[\"package_name\"],\n            database=node_values[\"database\"],\n            schema=node_values[\"schema\"],\n            alias=node_values[\"alias\"],\n            patch_path=node_values[\"patch_path\"],\n            tags=node_values[\"tags\"],\n            tests=[\n                Test.from_node(test)\n                for test in test_values\n                if not test.get(\"test_metadata\", {})\n                .get(\"kwargs\", {})\n                .get(\"column_name\")\n            ],\n            _raw_values=node_values,\n            _raw_test_values=test_values,\n        )\n\n    def __hash__(self) -&gt; int:\n        \"\"\"Compute a unique hash for a seed.\"\"\"\n        return hash(self.unique_id)\n</code></pre>"},{"location":"reference/models/#dbt_score.models.Seed.__hash__","title":"<code>__hash__()</code>","text":"<p>Compute a unique hash for a seed.</p> Source code in <code>src/dbt_score/models.py</code> <pre><code>def __hash__(self) -&gt; int:\n    \"\"\"Compute a unique hash for a seed.\"\"\"\n    return hash(self.unique_id)\n</code></pre>"},{"location":"reference/models/#dbt_score.models.Seed.from_node","title":"<code>from_node(node_values, test_values)</code>  <code>classmethod</code>","text":"<p>Create a seed object from a node and its tests in the manifest.</p> Source code in <code>src/dbt_score/models.py</code> <pre><code>@classmethod\ndef from_node(\n    cls, node_values: dict[str, Any], test_values: list[dict[str, Any]]\n) -&gt; \"Seed\":\n    \"\"\"Create a seed object from a node and its tests in the manifest.\"\"\"\n    return cls(\n        unique_id=node_values[\"unique_id\"],\n        name=node_values[\"name\"],\n        relation_name=node_values[\"relation_name\"],\n        description=node_values[\"description\"],\n        original_file_path=node_values[\"original_file_path\"],\n        config=node_values[\"config\"],\n        meta=node_values[\"meta\"],\n        columns=cls._get_columns(node_values, test_values),\n        package_name=node_values[\"package_name\"],\n        database=node_values[\"database\"],\n        schema=node_values[\"schema\"],\n        alias=node_values[\"alias\"],\n        patch_path=node_values[\"patch_path\"],\n        tags=node_values[\"tags\"],\n        tests=[\n            Test.from_node(test)\n            for test in test_values\n            if not test.get(\"test_metadata\", {})\n            .get(\"kwargs\", {})\n            .get(\"column_name\")\n        ],\n        _raw_values=node_values,\n        _raw_test_values=test_values,\n    )\n</code></pre>"},{"location":"reference/models/#dbt_score.models.Snapshot","title":"<code>Snapshot</code>  <code>dataclass</code>","text":"<p>               Bases: <code>HasColumnsMixin</code></p> <p>Represents a dbt snapshot.</p> <p>Attributes:</p> Name Type Description <code>unique_id</code> <code>str</code> <p>The id of the snapshot, e.g. <code>snapshot.package.snapshot_name</code>.</p> <code>name</code> <code>str</code> <p>The name of the snapshot.</p> <code>relation_name</code> <code>str</code> <p>The relation name of the snapshot,</p> <code>description</code> <code>str</code> <p>The full description of the snapshot.</p> <code>original_file_path</code> <code>str</code> <p>The sql path of the snapshot,</p> <code>config</code> <code>dict[str, Any]</code> <p>The config of the snapshot.</p> <code>meta</code> <code>dict[str, Any]</code> <p>The meta of the snapshot.</p> <code>columns</code> <code>list[Column]</code> <p>The list of columns of the snapshot.</p> <code>package_name</code> <code>str</code> <p>The package name of the snapshot.</p> <code>database</code> <code>str</code> <p>The database name of the snapshot.</p> <code>schema</code> <code>str</code> <p>The schema name of the snapshot.</p> <code>raw_code</code> <code>str</code> <p>The raw code of the snapshot.</p> <code>language</code> <code>str</code> <p>The language of the snapshot, e.g. sql.</p> <code>alias</code> <code>str | None</code> <p>The alias of the snapshot.</p> <code>patch_path</code> <code>str | None</code> <p>The yml path of the snapshot, e.g.</p> <code>`package</code> <code>str | None</code> <p>//snapshot_dir/dir/file.yml`.</p> <code>tags</code> <code>list[str]</code> <p>The list of tags attached to the snapshot.</p> <code>tests</code> <code>list[Test]</code> <p>The list of tests attached to the snapshot.</p> <code>depends_on</code> <code>dict[str, list[str]]</code> <p>Dictionary of models/sources/macros that the model depends on.</p> <code>strategy</code> <code>str | None</code> <p>The strategy of the snapshot.</p> <code>unique_key</code> <code>list[str] | None</code> <p>The unique key of the snapshot.</p> <code>parents</code> <code>list[ParentType]</code> <p>The list of models, sources, and snapshots this snapshot depends on.</p> <code>children</code> <code>list[ChildType]</code> <p>The list of models and snapshots that depend on this snapshot.</p> <code>_raw_values</code> <code>dict[str, Any]</code> <p>The raw values of the snapshot (node) in the manifest.</p> <code>_raw_test_values</code> <code>list[dict[str, Any]]</code> <p>The raw test values of the snapshot (node) in the manifest.</p> Source code in <code>src/dbt_score/models.py</code> <pre><code>@dataclass\nclass Snapshot(HasColumnsMixin):\n    \"\"\"Represents a dbt snapshot.\n\n    Attributes:\n        unique_id: The id of the snapshot, e.g. `snapshot.package.snapshot_name`.\n        name: The name of the snapshot.\n        relation_name: The relation name of the snapshot,\n        e.g. `db.schema.snapshot_name`.\n        description: The full description of the snapshot.\n        original_file_path: The sql path of the snapshot,\n        `e.g. snapshot_dir/dir/file.sql`.\n        config: The config of the snapshot.\n        meta: The meta of the snapshot.\n        columns: The list of columns of the snapshot.\n        package_name: The package name of the snapshot.\n        database: The database name of the snapshot.\n        schema: The schema name of the snapshot.\n        raw_code: The raw code of the snapshot.\n        language: The language of the snapshot, e.g. sql.\n        alias: The alias of the snapshot.\n        patch_path: The yml path of the snapshot, e.g.\n        `package://snapshot_dir/dir/file.yml`.\n        tags: The list of tags attached to the snapshot.\n        tests: The list of tests attached to the snapshot.\n        depends_on: Dictionary of models/sources/macros that the model depends on.\n        strategy: The strategy of the snapshot.\n        unique_key: The unique key of the snapshot.\n        parents: The list of models, sources, and snapshots this snapshot depends on.\n        children: The list of models and snapshots that depend on this snapshot.\n        _raw_values: The raw values of the snapshot (node) in the manifest.\n        _raw_test_values: The raw test values of the snapshot (node) in the manifest.\n    \"\"\"\n\n    unique_id: str\n    name: str\n    relation_name: str\n    description: str\n    original_file_path: str\n    config: dict[str, Any]\n    meta: dict[str, Any]\n    columns: list[Column]\n    package_name: str\n    database: str\n    schema: str\n    raw_code: str\n    language: str\n    alias: str | None = None\n    patch_path: str | None = None\n    tags: list[str] = field(default_factory=list)\n    tests: list[Test] = field(default_factory=list)\n    depends_on: dict[str, list[str]] = field(default_factory=dict)\n    strategy: str | None = None\n    unique_key: list[str] | None = None\n    parents: list[ParentType] = field(default_factory=list)\n    children: list[ChildType] = field(default_factory=list)\n    _raw_values: dict[str, Any] = field(default_factory=dict)\n    _raw_test_values: list[dict[str, Any]] = field(default_factory=list)\n\n    @classmethod\n    def from_node(\n        cls, node_values: dict[str, Any], test_values: list[dict[str, Any]]\n    ) -&gt; \"Snapshot\":\n        \"\"\"Create a snapshot object from a node and its tests in the manifest.\"\"\"\n        return cls(\n            unique_id=node_values[\"unique_id\"],\n            name=node_values[\"name\"],\n            relation_name=node_values[\"relation_name\"],\n            description=node_values[\"description\"],\n            original_file_path=node_values[\"original_file_path\"],\n            config=node_values[\"config\"],\n            meta=node_values[\"meta\"],\n            columns=cls._get_columns(node_values, test_values),\n            package_name=node_values[\"package_name\"],\n            database=node_values[\"database\"],\n            schema=node_values[\"schema\"],\n            raw_code=node_values[\"raw_code\"],\n            language=node_values[\"language\"],\n            alias=node_values[\"alias\"],\n            patch_path=node_values[\"patch_path\"],\n            tags=node_values[\"tags\"],\n            tests=[\n                Test.from_node(test)\n                for test in test_values\n                if not test.get(\"test_metadata\", {})\n                .get(\"kwargs\", {})\n                .get(\"column_name\")\n            ],\n            depends_on=node_values[\"depends_on\"],\n            parents=[],  # Will be populated later\n            _raw_values=node_values,\n            _raw_test_values=test_values,\n        )\n\n    def __hash__(self) -&gt; int:\n        \"\"\"Compute a unique hash for a snapshot.\"\"\"\n        return hash(self.unique_id)\n</code></pre>"},{"location":"reference/models/#dbt_score.models.Snapshot.__hash__","title":"<code>__hash__()</code>","text":"<p>Compute a unique hash for a snapshot.</p> Source code in <code>src/dbt_score/models.py</code> <pre><code>def __hash__(self) -&gt; int:\n    \"\"\"Compute a unique hash for a snapshot.\"\"\"\n    return hash(self.unique_id)\n</code></pre>"},{"location":"reference/models/#dbt_score.models.Snapshot.from_node","title":"<code>from_node(node_values, test_values)</code>  <code>classmethod</code>","text":"<p>Create a snapshot object from a node and its tests in the manifest.</p> Source code in <code>src/dbt_score/models.py</code> <pre><code>@classmethod\ndef from_node(\n    cls, node_values: dict[str, Any], test_values: list[dict[str, Any]]\n) -&gt; \"Snapshot\":\n    \"\"\"Create a snapshot object from a node and its tests in the manifest.\"\"\"\n    return cls(\n        unique_id=node_values[\"unique_id\"],\n        name=node_values[\"name\"],\n        relation_name=node_values[\"relation_name\"],\n        description=node_values[\"description\"],\n        original_file_path=node_values[\"original_file_path\"],\n        config=node_values[\"config\"],\n        meta=node_values[\"meta\"],\n        columns=cls._get_columns(node_values, test_values),\n        package_name=node_values[\"package_name\"],\n        database=node_values[\"database\"],\n        schema=node_values[\"schema\"],\n        raw_code=node_values[\"raw_code\"],\n        language=node_values[\"language\"],\n        alias=node_values[\"alias\"],\n        patch_path=node_values[\"patch_path\"],\n        tags=node_values[\"tags\"],\n        tests=[\n            Test.from_node(test)\n            for test in test_values\n            if not test.get(\"test_metadata\", {})\n            .get(\"kwargs\", {})\n            .get(\"column_name\")\n        ],\n        depends_on=node_values[\"depends_on\"],\n        parents=[],  # Will be populated later\n        _raw_values=node_values,\n        _raw_test_values=test_values,\n    )\n</code></pre>"},{"location":"reference/models/#dbt_score.models.Source","title":"<code>Source</code>  <code>dataclass</code>","text":"<p>               Bases: <code>HasColumnsMixin</code></p> <p>Represents a dbt source table.</p> <p>Attributes:</p> Name Type Description <code>unique_id</code> <code>str</code> <p>The id of the source table, e.g. 'source.package.source_name.source_table_name'.</p> <code>name</code> <code>str</code> <p>The alias of the source table.</p> <code>description</code> <code>str</code> <p>The full description of the source table.</p> <code>source_name</code> <code>str</code> <p>The source namespace.</p> <code>source_description</code> <code>str</code> <p>The description for the source namespace.</p> <code>original_file_path</code> <code>str</code> <p>The yml path to the source definition.</p> <code>config</code> <code>dict[str, Any]</code> <p>The config of the source definition.</p> <code>meta</code> <code>dict[str, Any]</code> <p>Any meta-attributes on the source table.</p> <code>source_meta</code> <code>dict[str, Any]</code> <p>Any meta-attribuets on the source namespace.</p> <code>columns</code> <code>list[Column]</code> <p>The list of columns for the source table.</p> <code>package_name</code> <code>str</code> <p>The dbt package name for the source table.</p> <code>database</code> <code>str</code> <p>The database name of the source table.</p> <code>schema</code> <code>str</code> <p>The schema name of the source table.</p> <code>identifier</code> <code>str</code> <p>The actual source table name, i.e. not an alias.</p> <code>loader</code> <code>str</code> <p>The tool used to load the source table into the warehouse.</p> <code>freshness</code> <code>SourceFreshness</code> <p>A set of time thresholds after which data may be considered stale.</p> <code>patch_path</code> <code>str | None</code> <p>The yml path of the source definition.</p> <code>tags</code> <code>list[str]</code> <p>The list of tags attached to the source table.</p> <code>tests</code> <code>list[Test]</code> <p>The list of tests attached to the source table.</p> <code>children</code> <code>list[ChildType]</code> <p>The list of models and snapshots that depend on this source.</p> <code>_raw_values</code> <code>dict[str, Any]</code> <p>The raw values of the source definition in the manifest.</p> <code>_raw_test_values</code> <code>list[dict[str, Any]]</code> <p>The raw test values of the source definition in the manifest.</p> Source code in <code>src/dbt_score/models.py</code> <pre><code>@dataclass\nclass Source(HasColumnsMixin):\n    \"\"\"Represents a dbt source table.\n\n    Attributes:\n        unique_id: The id of the source table,\n            e.g. 'source.package.source_name.source_table_name'.\n        name: The alias of the source table.\n        description: The full description of the source table.\n        source_name: The source namespace.\n        source_description: The description for the source namespace.\n        original_file_path: The yml path to the source definition.\n        config: The config of the source definition.\n        meta: Any meta-attributes on the source table.\n        source_meta: Any meta-attribuets on the source namespace.\n        columns: The list of columns for the source table.\n        package_name: The dbt package name for the source table.\n        database: The database name of the source table.\n        schema: The schema name of the source table.\n        identifier: The actual source table name, i.e. not an alias.\n        loader: The tool used to load the source table into the warehouse.\n        freshness: A set of time thresholds after which data may be considered stale.\n        patch_path: The yml path of the source definition.\n        tags: The list of tags attached to the source table.\n        tests: The list of tests attached to the source table.\n        children: The list of models and snapshots that depend on this source.\n        _raw_values: The raw values of the source definition in the manifest.\n        _raw_test_values: The raw test values of the source definition in the manifest.\n    \"\"\"\n\n    unique_id: str\n    name: str\n    description: str\n    source_name: str\n    source_description: str\n    original_file_path: str\n    config: dict[str, Any]\n    meta: dict[str, Any]\n    source_meta: dict[str, Any]\n    columns: list[Column]\n    package_name: str\n    database: str\n    schema: str\n    identifier: str\n    loader: str\n    freshness: SourceFreshness\n    patch_path: str | None = None\n    tags: list[str] = field(default_factory=list)\n    tests: list[Test] = field(default_factory=list)\n    children: list[ChildType] = field(default_factory=list)\n    _raw_values: dict[str, Any] = field(default_factory=dict)\n    _raw_test_values: list[dict[str, Any]] = field(default_factory=list)\n\n    @property\n    def selector_name(self) -&gt; str:\n        \"\"\"Returns the name used by the dbt `source` method selector.\n\n        Note: This is also the format output by `dbt ls --output name` for sources.\n\n        https://docs.getdbt.com/reference/node-selection/methods#the-source-method\n        \"\"\"\n        return f\"{self.source_name}.{self.name}\"\n\n    @classmethod\n    def from_node(\n        cls, node_values: dict[str, Any], test_values: list[dict[str, Any]]\n    ) -&gt; \"Source\":\n        \"\"\"Create a source object from a node and it's tests in the manifest.\"\"\"\n        return cls(\n            unique_id=node_values[\"unique_id\"],\n            name=node_values[\"name\"],\n            description=node_values[\"description\"],\n            source_name=node_values[\"source_name\"],\n            source_description=node_values[\"source_description\"],\n            original_file_path=node_values[\"original_file_path\"],\n            config=node_values[\"config\"],\n            meta=node_values[\"meta\"],\n            source_meta=node_values[\"source_meta\"],\n            columns=cls._get_columns(node_values, test_values),\n            package_name=node_values[\"package_name\"],\n            database=node_values[\"database\"],\n            schema=node_values[\"schema\"],\n            identifier=node_values[\"identifier\"],\n            loader=node_values[\"loader\"],\n            freshness=node_values[\"freshness\"],\n            patch_path=node_values[\"patch_path\"],\n            tags=node_values[\"tags\"],\n            tests=[\n                Test.from_node(test)\n                for test in test_values\n                if not test.get(\"test_metadata\", {})\n                .get(\"kwargs\", {})\n                .get(\"column_name\")\n            ],\n            _raw_values=node_values,\n            _raw_test_values=test_values,\n        )\n\n    def __hash__(self) -&gt; int:\n        \"\"\"Compute a unique hash for a source.\"\"\"\n        return hash(self.unique_id)\n</code></pre>"},{"location":"reference/models/#dbt_score.models.Source.selector_name","title":"<code>selector_name</code>  <code>property</code>","text":"<p>Returns the name used by the dbt <code>source</code> method selector.</p> <p>Note: This is also the format output by <code>dbt ls --output name</code> for sources.</p> <p>https://docs.getdbt.com/reference/node-selection/methods#the-source-method</p>"},{"location":"reference/models/#dbt_score.models.Source.__hash__","title":"<code>__hash__()</code>","text":"<p>Compute a unique hash for a source.</p> Source code in <code>src/dbt_score/models.py</code> <pre><code>def __hash__(self) -&gt; int:\n    \"\"\"Compute a unique hash for a source.\"\"\"\n    return hash(self.unique_id)\n</code></pre>"},{"location":"reference/models/#dbt_score.models.Source.from_node","title":"<code>from_node(node_values, test_values)</code>  <code>classmethod</code>","text":"<p>Create a source object from a node and it's tests in the manifest.</p> Source code in <code>src/dbt_score/models.py</code> <pre><code>@classmethod\ndef from_node(\n    cls, node_values: dict[str, Any], test_values: list[dict[str, Any]]\n) -&gt; \"Source\":\n    \"\"\"Create a source object from a node and it's tests in the manifest.\"\"\"\n    return cls(\n        unique_id=node_values[\"unique_id\"],\n        name=node_values[\"name\"],\n        description=node_values[\"description\"],\n        source_name=node_values[\"source_name\"],\n        source_description=node_values[\"source_description\"],\n        original_file_path=node_values[\"original_file_path\"],\n        config=node_values[\"config\"],\n        meta=node_values[\"meta\"],\n        source_meta=node_values[\"source_meta\"],\n        columns=cls._get_columns(node_values, test_values),\n        package_name=node_values[\"package_name\"],\n        database=node_values[\"database\"],\n        schema=node_values[\"schema\"],\n        identifier=node_values[\"identifier\"],\n        loader=node_values[\"loader\"],\n        freshness=node_values[\"freshness\"],\n        patch_path=node_values[\"patch_path\"],\n        tags=node_values[\"tags\"],\n        tests=[\n            Test.from_node(test)\n            for test in test_values\n            if not test.get(\"test_metadata\", {})\n            .get(\"kwargs\", {})\n            .get(\"column_name\")\n        ],\n        _raw_values=node_values,\n        _raw_test_values=test_values,\n    )\n</code></pre>"},{"location":"reference/models/#dbt_score.models.SourceFreshness","title":"<code>SourceFreshness</code>  <code>dataclass</code>","text":"<p>Represents a source freshness configuration.</p> <p>This is referred to as <code>FreshnessThreshold</code> in the dbt JSONSchema.</p> <p>Attributes:</p> Name Type Description <code>warn_after</code> <code>Duration</code> <p>The threshold after which the dbt source freshness check should soft-fail with a warning.</p> <code>error_after</code> <code>Duration</code> <p>The threshold after which the dbt source freshness check should fail.</p> <code>filter</code> <code>str | None</code> <p>An optional filter to apply to the input data before running source freshness check.</p> Source code in <code>src/dbt_score/models.py</code> <pre><code>@dataclass\nclass SourceFreshness:\n    \"\"\"Represents a source freshness configuration.\n\n    This is referred to as `FreshnessThreshold` in the dbt JSONSchema.\n\n    Attributes:\n        warn_after: The threshold after which the dbt source freshness check should\n            soft-fail with a warning.\n        error_after: The threshold after which the dbt source freshness check should\n            fail.\n        filter: An optional filter to apply to the input data before running\n            source freshness check.\n    \"\"\"\n\n    warn_after: Duration\n    error_after: Duration\n    filter: str | None = None\n</code></pre>"},{"location":"reference/models/#dbt_score.models.Test","title":"<code>Test</code>  <code>dataclass</code>","text":"<p>Test for a column, model, source or snapshot.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the test.</p> <code>type</code> <code>str</code> <p>The type of the test, e.g. <code>unique</code>.</p> <code>kwargs</code> <code>dict[str, Any]</code> <p>The kwargs of the test.</p> <code>tags</code> <code>list[str]</code> <p>The list of tags attached to the test.</p> <code>_raw_values</code> <code>dict[str, Any]</code> <p>The raw values of the test in the manifest.</p> Source code in <code>src/dbt_score/models.py</code> <pre><code>@dataclass\nclass Test:\n    \"\"\"Test for a column, model, source or snapshot.\n\n    Attributes:\n        name: The name of the test.\n        type: The type of the test, e.g. `unique`.\n        kwargs: The kwargs of the test.\n        tags: The list of tags attached to the test.\n        _raw_values: The raw values of the test in the manifest.\n    \"\"\"\n\n    name: str\n    type: str\n    kwargs: dict[str, Any] = field(default_factory=dict)\n    tags: list[str] = field(default_factory=list)\n    _raw_values: dict[str, Any] = field(default_factory=dict)\n\n    @classmethod\n    def from_node(cls, test_node: dict[str, Any]) -&gt; \"Test\":\n        \"\"\"Create a test object from a test node in the manifest.\"\"\"\n        return cls(\n            name=test_node[\"name\"],\n            type=test_node.get(\"test_metadata\", {}).get(\"name\", \"generic\"),\n            kwargs=test_node.get(\"test_metadata\", {}).get(\"kwargs\", {}),\n            tags=test_node.get(\"tags\", []),\n            _raw_values=test_node,\n        )\n</code></pre>"},{"location":"reference/models/#dbt_score.models.Test.from_node","title":"<code>from_node(test_node)</code>  <code>classmethod</code>","text":"<p>Create a test object from a test node in the manifest.</p> Source code in <code>src/dbt_score/models.py</code> <pre><code>@classmethod\ndef from_node(cls, test_node: dict[str, Any]) -&gt; \"Test\":\n    \"\"\"Create a test object from a test node in the manifest.\"\"\"\n    return cls(\n        name=test_node[\"name\"],\n        type=test_node.get(\"test_metadata\", {}).get(\"name\", \"generic\"),\n        kwargs=test_node.get(\"test_metadata\", {}).get(\"kwargs\", {}),\n        tags=test_node.get(\"tags\", []),\n        _raw_values=test_node,\n    )\n</code></pre>"},{"location":"reference/rule/","title":"Rule","text":"<p>Rule definitions.</p>"},{"location":"reference/rule/#dbt_score.rule.Rule","title":"<code>Rule</code>","text":"<p>The rule base class.</p> Source code in <code>src/dbt_score/rule.py</code> <pre><code>class Rule:\n    \"\"\"The rule base class.\"\"\"\n\n    description: str\n    severity: Severity = Severity.MEDIUM\n    rule_filter_names: list[str]\n    rule_filters: frozenset[RuleFilter] = frozenset()\n    default_config: typing.ClassVar[dict[str, Any]] = {}\n    resource_type: typing.ClassVar[type[Evaluable]]\n\n    def __init__(self, rule_config: RuleConfig | None = None) -&gt; None:\n        \"\"\"Initialize the rule.\"\"\"\n        self.config: dict[str, Any] = {}\n        if rule_config:\n            self.process_config(rule_config)\n\n    def __init_subclass__(cls, **kwargs) -&gt; None:  # type: ignore\n        \"\"\"Initializes the subclass.\"\"\"\n        super().__init_subclass__(**kwargs)\n        if not hasattr(cls, \"description\"):\n            raise AttributeError(\"Subclass must define class attribute `description`.\")\n\n        cls.resource_type = cls._introspect_resource_type()\n\n        cls._validate_rule_filters()\n\n    @classmethod\n    def _validate_rule_filters(cls) -&gt; None:\n        for rule_filter in cls.rule_filters:\n            if rule_filter.resource_type != cls.resource_type:\n                raise TypeError(\n                    f\"Mismatched resource_type on filter \"\n                    f\"{rule_filter.__class__.__name__}. \"\n                    f\"Expected {cls.resource_type.__name__}, \"\n                    f\"but got {rule_filter.resource_type.__name__}.\"\n                )\n\n    @classmethod\n    def _introspect_resource_type(cls) -&gt; Type[Evaluable]:\n        evaluate_func = getattr(cls, \"_orig_evaluate\", cls.evaluate)\n\n        sig = inspect.signature(evaluate_func)\n        resource_type_argument = first_true(\n            sig.parameters.values(),\n            pred=lambda arg: arg.annotation in typing.get_args(Evaluable),\n        )\n\n        if not resource_type_argument:\n            raise TypeError(\n                \"Subclass must implement method `evaluate` with an \"\n                \"annotated Model or Source argument.\"\n            )\n\n        resource_type = cast(type[Evaluable], resource_type_argument.annotation)\n        return resource_type\n\n    def process_config(self, rule_config: RuleConfig) -&gt; None:\n        \"\"\"Process the rule config.\"\"\"\n        config = self.default_config.copy()\n\n        # Overwrite default rule configuration\n        for k, v in rule_config.config.items():\n            if k in self.default_config:\n                config[k] = v\n            else:\n                raise AttributeError(\n                    f\"Unknown rule parameter: {k} for rule {self.source()}.\"\n                )\n\n        self.set_severity(\n            rule_config.severity\n        ) if rule_config.severity else rule_config.severity\n        self.rule_filter_names = rule_config.rule_filter_names\n        self.config = config\n\n    def evaluate(self, evaluable: Evaluable) -&gt; RuleViolation | None:\n        \"\"\"Evaluates the rule.\"\"\"\n        raise NotImplementedError(\"Subclass must implement method `evaluate`.\")\n\n    @classmethod\n    def should_evaluate(cls, evaluable: Evaluable) -&gt; bool:\n        \"\"\"Checks whether the rule should be applied against the evaluable.\n\n        The evaluable must satisfy the following criteria:\n            - all filters in the rule allow evaluation\n            - the rule and evaluable have matching resource_types\n        \"\"\"\n        resource_types_match = cls.resource_type is type(evaluable)\n\n        if not resource_types_match:\n            return False\n\n        if cls.rule_filters:\n            return all(f.evaluate(evaluable) for f in cls.rule_filters)\n\n        return True\n\n    @classmethod\n    def set_severity(cls, severity: Severity) -&gt; None:\n        \"\"\"Set the severity of the rule.\"\"\"\n        cls.severity = severity\n\n    @classmethod\n    def set_filters(cls, rule_filters: Iterable[RuleFilter]) -&gt; None:\n        \"\"\"Set the filters of the rule.\"\"\"\n        cls.rule_filters = frozenset(rule_filters)\n\n    @classmethod\n    def source(cls) -&gt; str:\n        \"\"\"Return the source of the rule, i.e. a fully qualified name.\"\"\"\n        return f\"{cls.__module__}.{cls.__name__}\"\n\n    def __hash__(self) -&gt; int:\n        \"\"\"Compute a unique hash for a rule.\"\"\"\n        return hash(self.source())\n</code></pre>"},{"location":"reference/rule/#dbt_score.rule.Rule.__hash__","title":"<code>__hash__()</code>","text":"<p>Compute a unique hash for a rule.</p> Source code in <code>src/dbt_score/rule.py</code> <pre><code>def __hash__(self) -&gt; int:\n    \"\"\"Compute a unique hash for a rule.\"\"\"\n    return hash(self.source())\n</code></pre>"},{"location":"reference/rule/#dbt_score.rule.Rule.__init__","title":"<code>__init__(rule_config=None)</code>","text":"<p>Initialize the rule.</p> Source code in <code>src/dbt_score/rule.py</code> <pre><code>def __init__(self, rule_config: RuleConfig | None = None) -&gt; None:\n    \"\"\"Initialize the rule.\"\"\"\n    self.config: dict[str, Any] = {}\n    if rule_config:\n        self.process_config(rule_config)\n</code></pre>"},{"location":"reference/rule/#dbt_score.rule.Rule.__init_subclass__","title":"<code>__init_subclass__(**kwargs)</code>","text":"<p>Initializes the subclass.</p> Source code in <code>src/dbt_score/rule.py</code> <pre><code>def __init_subclass__(cls, **kwargs) -&gt; None:  # type: ignore\n    \"\"\"Initializes the subclass.\"\"\"\n    super().__init_subclass__(**kwargs)\n    if not hasattr(cls, \"description\"):\n        raise AttributeError(\"Subclass must define class attribute `description`.\")\n\n    cls.resource_type = cls._introspect_resource_type()\n\n    cls._validate_rule_filters()\n</code></pre>"},{"location":"reference/rule/#dbt_score.rule.Rule.evaluate","title":"<code>evaluate(evaluable)</code>","text":"<p>Evaluates the rule.</p> Source code in <code>src/dbt_score/rule.py</code> <pre><code>def evaluate(self, evaluable: Evaluable) -&gt; RuleViolation | None:\n    \"\"\"Evaluates the rule.\"\"\"\n    raise NotImplementedError(\"Subclass must implement method `evaluate`.\")\n</code></pre>"},{"location":"reference/rule/#dbt_score.rule.Rule.process_config","title":"<code>process_config(rule_config)</code>","text":"<p>Process the rule config.</p> Source code in <code>src/dbt_score/rule.py</code> <pre><code>def process_config(self, rule_config: RuleConfig) -&gt; None:\n    \"\"\"Process the rule config.\"\"\"\n    config = self.default_config.copy()\n\n    # Overwrite default rule configuration\n    for k, v in rule_config.config.items():\n        if k in self.default_config:\n            config[k] = v\n        else:\n            raise AttributeError(\n                f\"Unknown rule parameter: {k} for rule {self.source()}.\"\n            )\n\n    self.set_severity(\n        rule_config.severity\n    ) if rule_config.severity else rule_config.severity\n    self.rule_filter_names = rule_config.rule_filter_names\n    self.config = config\n</code></pre>"},{"location":"reference/rule/#dbt_score.rule.Rule.set_filters","title":"<code>set_filters(rule_filters)</code>  <code>classmethod</code>","text":"<p>Set the filters of the rule.</p> Source code in <code>src/dbt_score/rule.py</code> <pre><code>@classmethod\ndef set_filters(cls, rule_filters: Iterable[RuleFilter]) -&gt; None:\n    \"\"\"Set the filters of the rule.\"\"\"\n    cls.rule_filters = frozenset(rule_filters)\n</code></pre>"},{"location":"reference/rule/#dbt_score.rule.Rule.set_severity","title":"<code>set_severity(severity)</code>  <code>classmethod</code>","text":"<p>Set the severity of the rule.</p> Source code in <code>src/dbt_score/rule.py</code> <pre><code>@classmethod\ndef set_severity(cls, severity: Severity) -&gt; None:\n    \"\"\"Set the severity of the rule.\"\"\"\n    cls.severity = severity\n</code></pre>"},{"location":"reference/rule/#dbt_score.rule.Rule.should_evaluate","title":"<code>should_evaluate(evaluable)</code>  <code>classmethod</code>","text":"<p>Checks whether the rule should be applied against the evaluable.</p> The evaluable must satisfy the following criteria <ul> <li>all filters in the rule allow evaluation</li> <li>the rule and evaluable have matching resource_types</li> </ul> Source code in <code>src/dbt_score/rule.py</code> <pre><code>@classmethod\ndef should_evaluate(cls, evaluable: Evaluable) -&gt; bool:\n    \"\"\"Checks whether the rule should be applied against the evaluable.\n\n    The evaluable must satisfy the following criteria:\n        - all filters in the rule allow evaluation\n        - the rule and evaluable have matching resource_types\n    \"\"\"\n    resource_types_match = cls.resource_type is type(evaluable)\n\n    if not resource_types_match:\n        return False\n\n    if cls.rule_filters:\n        return all(f.evaluate(evaluable) for f in cls.rule_filters)\n\n    return True\n</code></pre>"},{"location":"reference/rule/#dbt_score.rule.Rule.source","title":"<code>source()</code>  <code>classmethod</code>","text":"<p>Return the source of the rule, i.e. a fully qualified name.</p> Source code in <code>src/dbt_score/rule.py</code> <pre><code>@classmethod\ndef source(cls) -&gt; str:\n    \"\"\"Return the source of the rule, i.e. a fully qualified name.\"\"\"\n    return f\"{cls.__module__}.{cls.__name__}\"\n</code></pre>"},{"location":"reference/rule/#dbt_score.rule.RuleConfig","title":"<code>RuleConfig</code>  <code>dataclass</code>","text":"<p>Configuration for a rule.</p> Source code in <code>src/dbt_score/rule.py</code> <pre><code>@dataclass\nclass RuleConfig:\n    \"\"\"Configuration for a rule.\"\"\"\n\n    severity: Severity | None = None\n    config: dict[str, Any] = field(default_factory=dict)\n    rule_filter_names: list[str] = field(default_factory=list)\n\n    @staticmethod\n    def from_dict(rule_config: dict[str, Any]) -&gt; \"RuleConfig\":\n        \"\"\"Create a RuleConfig from a dictionary.\"\"\"\n        config = rule_config.copy()\n        severity = (\n            Severity(config.pop(\"severity\", None))\n            if \"severity\" in rule_config\n            else None\n        )\n        filter_names = (\n            config.pop(\"rule_filter_names\", None)\n            if \"rule_filter_names\" in rule_config\n            else []\n        )\n\n        return RuleConfig(\n            severity=severity, config=config, rule_filter_names=filter_names\n        )\n</code></pre>"},{"location":"reference/rule/#dbt_score.rule.RuleConfig.from_dict","title":"<code>from_dict(rule_config)</code>  <code>staticmethod</code>","text":"<p>Create a RuleConfig from a dictionary.</p> Source code in <code>src/dbt_score/rule.py</code> <pre><code>@staticmethod\ndef from_dict(rule_config: dict[str, Any]) -&gt; \"RuleConfig\":\n    \"\"\"Create a RuleConfig from a dictionary.\"\"\"\n    config = rule_config.copy()\n    severity = (\n        Severity(config.pop(\"severity\", None))\n        if \"severity\" in rule_config\n        else None\n    )\n    filter_names = (\n        config.pop(\"rule_filter_names\", None)\n        if \"rule_filter_names\" in rule_config\n        else []\n    )\n\n    return RuleConfig(\n        severity=severity, config=config, rule_filter_names=filter_names\n    )\n</code></pre>"},{"location":"reference/rule/#dbt_score.rule.RuleViolation","title":"<code>RuleViolation</code>  <code>dataclass</code>","text":"<p>The violation of a rule.</p> Source code in <code>src/dbt_score/rule.py</code> <pre><code>@dataclass\nclass RuleViolation:\n    \"\"\"The violation of a rule.\"\"\"\n\n    message: str | None = None\n</code></pre>"},{"location":"reference/rule/#dbt_score.rule.Severity","title":"<code>Severity</code>","text":"<p>               Bases: <code>Enum</code></p> <p>The severity/weight of a rule.</p> Source code in <code>src/dbt_score/rule.py</code> <pre><code>class Severity(Enum):\n    \"\"\"The severity/weight of a rule.\"\"\"\n\n    LOW = 1\n    MEDIUM = 2\n    HIGH = 3\n    CRITICAL = 4\n</code></pre>"},{"location":"reference/rule/#dbt_score.rule.rule","title":"<code>rule(__func=None, *, description=None, severity=Severity.MEDIUM, rule_filters=None)</code>","text":"<pre><code>rule(__func: ModelRuleEvaluationType) -&gt; Type[Rule]\n</code></pre><pre><code>rule(__func: SourceRuleEvaluationType) -&gt; Type[Rule]\n</code></pre><pre><code>rule(__func: SnapshotRuleEvaluationType) -&gt; Type[Rule]\n</code></pre><pre><code>rule(__func: ExposureRuleEvaluationType) -&gt; Type[Rule]\n</code></pre><pre><code>rule(__func: SeedRuleEvaluationType) -&gt; Type[Rule]\n</code></pre><pre><code>rule(*, description: str | None = None, severity: Severity = Severity.MEDIUM, rule_filters: set[RuleFilter] | None = None) -&gt; Callable[[RuleEvaluationType], Type[Rule]]\n</code></pre> <p>Rule decorator.</p> <p>The rule decorator creates a rule class (subclass of Rule) and returns it.</p> <p>Using arguments or not are both supported: - <code>@rule</code> - <code>@rule(description=\"...\")</code></p> <p>Parameters:</p> Name Type Description Default <code>__func</code> <code>RuleEvaluationType | None</code> <p>The rule evaluation function being decorated.</p> <code>None</code> <code>description</code> <code>str | None</code> <p>The description of the rule.</p> <code>None</code> <code>severity</code> <code>Severity</code> <p>The severity of the rule.</p> <code>MEDIUM</code> <code>rule_filters</code> <code>set[RuleFilter] | None</code> <p>Set of RuleFilter that filters the items that the rule applies to.</p> <code>None</code> Source code in <code>src/dbt_score/rule.py</code> <pre><code>def rule(\n    __func: RuleEvaluationType | None = None,\n    *,\n    description: str | None = None,\n    severity: Severity = Severity.MEDIUM,\n    rule_filters: set[RuleFilter] | None = None,\n) -&gt; Type[Rule] | Callable[[RuleEvaluationType], Type[Rule]]:\n    \"\"\"Rule decorator.\n\n    The rule decorator creates a rule class (subclass of Rule) and returns it.\n\n    Using arguments or not are both supported:\n    - ``@rule``\n    - ``@rule(description=\"...\")``\n\n    Args:\n        __func: The rule evaluation function being decorated.\n        description: The description of the rule.\n        severity: The severity of the rule.\n        rule_filters: Set of RuleFilter that filters the items that the rule applies to.\n    \"\"\"\n\n    def decorator_rule(func: RuleEvaluationType) -&gt; Type[Rule]:\n        \"\"\"Decorator function.\"\"\"\n        if func.__doc__ is None and description is None:\n            raise AttributeError(\"Rule must define `description` or `func.__doc__`.\")\n\n        # Get description parameter, otherwise use the docstring\n        rule_description = description or (\n            func.__doc__.split(\"\\n\")[0] if func.__doc__ else None\n        )\n\n        def wrapped_func(self: Rule, *args: Any, **kwargs: Any) -&gt; RuleViolation | None:\n            \"\"\"Wrap func to add `self`.\"\"\"\n            return func(*args, **kwargs)\n\n        # Get default parameters from the rule definition\n        default_config = {\n            key: val.default\n            for key, val in inspect.signature(func).parameters.items()\n            if val.default != inspect.Parameter.empty\n        }\n\n        # Create the rule class inheriting from Rule\n        rule_class = type(\n            func.__name__,\n            (Rule,),\n            {\n                \"description\": rule_description,\n                \"severity\": severity,\n                \"rule_filters\": rule_filters or frozenset(),\n                \"default_config\": default_config,\n                \"evaluate\": wrapped_func,\n                # Save provided evaluate function\n                \"_orig_evaluate\": func,\n                # Forward origin of the decorated function\n                \"__qualname__\": func.__qualname__,  # https://peps.python.org/pep-3155/\n                \"__module__\": func.__module__,\n            },\n        )\n\n        return rule_class\n\n    if __func is not None:\n        # The syntax @rule is used\n        return decorator_rule(__func)\n    else:\n        # The syntax @rule(...) is used\n        return decorator_rule\n</code></pre>"},{"location":"reference/rule_registry/","title":"Rule registry","text":"<p>Rule registry.</p> <p>This module implements rule discovery.</p>"},{"location":"reference/rule_registry/#dbt_score.rule_registry.RuleRegistry","title":"<code>RuleRegistry</code>","text":"<p>A container for configured rules.</p> Source code in <code>src/dbt_score/rule_registry.py</code> <pre><code>class RuleRegistry:\n    \"\"\"A container for configured rules.\"\"\"\n\n    def __init__(self, config: Config) -&gt; None:\n        \"\"\"Instantiate a rule registry.\"\"\"\n        self.config = config\n        self._rules: dict[str, Rule] = {}\n        self._rule_filters: dict[str, RuleFilter] = {}\n\n    @property\n    def rules(self) -&gt; dict[str, Rule]:\n        \"\"\"Get all rules.\"\"\"\n        return self._rules\n\n    @property\n    def rule_filters(self) -&gt; dict[str, RuleFilter]:\n        \"\"\"Get all filters.\"\"\"\n        return self._rule_filters\n\n    def _walk_packages(self, namespace_name: str) -&gt; Iterator[str]:\n        \"\"\"Walk packages and sub-packages recursively.\"\"\"\n        try:\n            namespace = importlib.import_module(namespace_name)\n        except ImportError:  # no custom rule in Python path\n            if namespace_name != \"dbt_score_rules\":\n                logger.warning(f\"Can't import {namespace_name}.\")\n            return\n\n        if not hasattr(namespace, \"__path__\"):\n            # When called with a leaf, i.e. a module, don't attempt to iterate\n            yield namespace_name\n            return\n\n        for package in pkgutil.walk_packages(\n            namespace.__path__, namespace.__name__ + \".\"\n        ):\n            yield package.name\n\n    def _load(self, namespace_name: str) -&gt; None:\n        \"\"\"Load rules and filters found in a given namespace.\"\"\"\n        for module_name in self._walk_packages(namespace_name):\n            module = importlib.import_module(module_name)\n            for obj_name in dir(module):\n                obj = module.__dict__[obj_name]\n                # Skip adding objects imported from other modules\n                if type(obj) is type and module.__name__ != obj.__module__:\n                    continue\n                if type(obj) is type and issubclass(obj, Rule) and obj is not Rule:\n                    self._add_rule(obj)\n                if (\n                    type(obj) is type\n                    and issubclass(obj, RuleFilter)\n                    and obj is not RuleFilter\n                ):\n                    self._add_filter(obj)\n\n    def _add_rule(self, rule: Type[Rule]) -&gt; None:\n        \"\"\"Initialize and add a rule.\"\"\"\n        rule_name = rule.source()\n        if rule_name in self._rules:\n            raise DuplicatedRuleException(rule_name)\n        if rule_name not in self.config.disabled_rules:\n            rule_config = self.config.rules_config.get(rule_name, RuleConfig())\n            self._rules[rule_name] = rule(rule_config=rule_config)\n\n    def _add_filter(self, rule_filter: Type[RuleFilter]) -&gt; None:\n        \"\"\"Initialize and add a filter.\"\"\"\n        filter_name = rule_filter.source()\n        if filter_name in self._rule_filters:\n            raise DuplicatedRuleException(filter_name)\n        self._rule_filters[filter_name] = rule_filter()\n\n    def load_all(self) -&gt; None:\n        \"\"\"Load all rules, core and third-party.\"\"\"\n        # Add cwd to Python path\n        old_sys_path = sys.path  # Save original values\n        if self.config.inject_cwd_in_python_path and os.getcwd() not in sys.path:\n            sys.path.append(os.getcwd())\n\n        for namespace in self.config.rule_namespaces:\n            self._load(namespace)\n\n        # Restore original values\n        sys.path = old_sys_path\n\n        self._load_filters_into_rules()\n\n    def _load_filters_into_rules(self) -&gt; None:\n        \"\"\"Loads RuleFilters into Rule objects.\n\n        If the config of the rule has filter names in the `rule_filter_names` key,\n        load those filters from the rule registry into the actual `rule_filters` field.\n        Configuration overwrites any pre-existing filters.\n        \"\"\"\n        for rule in self._rules.values():\n            filter_names: list[str] = rule.rule_filter_names or []\n            if len(filter_names) &gt; 0:\n                rule.set_filters(\n                    rule_filter\n                    for name, rule_filter in self.rule_filters.items()\n                    if name in filter_names\n                )\n</code></pre>"},{"location":"reference/rule_registry/#dbt_score.rule_registry.RuleRegistry.rule_filters","title":"<code>rule_filters</code>  <code>property</code>","text":"<p>Get all filters.</p>"},{"location":"reference/rule_registry/#dbt_score.rule_registry.RuleRegistry.rules","title":"<code>rules</code>  <code>property</code>","text":"<p>Get all rules.</p>"},{"location":"reference/rule_registry/#dbt_score.rule_registry.RuleRegistry.__init__","title":"<code>__init__(config)</code>","text":"<p>Instantiate a rule registry.</p> Source code in <code>src/dbt_score/rule_registry.py</code> <pre><code>def __init__(self, config: Config) -&gt; None:\n    \"\"\"Instantiate a rule registry.\"\"\"\n    self.config = config\n    self._rules: dict[str, Rule] = {}\n    self._rule_filters: dict[str, RuleFilter] = {}\n</code></pre>"},{"location":"reference/rule_registry/#dbt_score.rule_registry.RuleRegistry.load_all","title":"<code>load_all()</code>","text":"<p>Load all rules, core and third-party.</p> Source code in <code>src/dbt_score/rule_registry.py</code> <pre><code>def load_all(self) -&gt; None:\n    \"\"\"Load all rules, core and third-party.\"\"\"\n    # Add cwd to Python path\n    old_sys_path = sys.path  # Save original values\n    if self.config.inject_cwd_in_python_path and os.getcwd() not in sys.path:\n        sys.path.append(os.getcwd())\n\n    for namespace in self.config.rule_namespaces:\n        self._load(namespace)\n\n    # Restore original values\n    sys.path = old_sys_path\n\n    self._load_filters_into_rules()\n</code></pre>"},{"location":"reference/scoring/","title":"Scoring","text":"<p>Module computing scores.</p>"},{"location":"reference/scoring/#dbt_score.scoring.Score","title":"<code>Score</code>  <code>dataclass</code>","text":"<p>Class representing a score.</p> Source code in <code>src/dbt_score/scoring.py</code> <pre><code>@dataclass\nclass Score:\n    \"\"\"Class representing a score.\"\"\"\n\n    value: float\n    badge: str\n\n    @property\n    def rounded_value(self) -&gt; float:\n        \"\"\"Auto-round score down to 1 decimal place.\"\"\"\n        return math.floor(self.value * 10) / 10\n</code></pre>"},{"location":"reference/scoring/#dbt_score.scoring.Score.rounded_value","title":"<code>rounded_value</code>  <code>property</code>","text":"<p>Auto-round score down to 1 decimal place.</p>"},{"location":"reference/scoring/#dbt_score.scoring.Scorer","title":"<code>Scorer</code>","text":"<p>Logic for computing scores.</p> Source code in <code>src/dbt_score/scoring.py</code> <pre><code>class Scorer:\n    \"\"\"Logic for computing scores.\"\"\"\n\n    # This magic number comes from rule severity.\n    # Assuming a rule violation:\n    # - A low severity yields a score 2/3\n    # - A medium severity yields a score 1/3\n    # - A high severity yields a score 0/3\n    score_cardinality = 3\n\n    min_score = 0.0\n    max_score = 10.0\n\n    def __init__(self, config: Config) -&gt; None:\n        \"\"\"Create a Scorer object.\"\"\"\n        self._config = config\n\n    def score_evaluable(self, evaluable_results: EvaluableResultsType) -&gt; Score:\n        \"\"\"Compute the score of a given evaluable.\"\"\"\n        rule_count = len(evaluable_results)\n\n        if rule_count == 0:\n            # No rule? No problem\n            score = self.max_score\n        elif any(\n            rule.severity == Severity.CRITICAL and isinstance(result, RuleViolation)\n            for rule, result in evaluable_results.items()\n        ):\n            # If there's a CRITICAL violation, the score is 0\n            score = self.min_score\n        else:\n            # Otherwise, the score is the weighted average (by severity) of the results\n            score = (\n                sum(\n                    [\n                        # The more severe the violation, the more points are lost\n                        self.score_cardinality - rule.severity.value\n                        if isinstance(result, RuleViolation)  # Either 0/3, 1/3 or 2/3\n                        else self.score_cardinality  # 3/3\n                        for rule, result in evaluable_results.items()\n                    ]\n                )\n                / (self.score_cardinality * rule_count)\n                * self.max_score\n            )\n\n        return Score(score, self._badge(score))\n\n    def score_aggregate_evaluables(self, scores: list[Score]) -&gt; Score:\n        \"\"\"Compute the score of a list of evaluables.\"\"\"\n        actual_scores = [s.value for s in scores]\n        if 0.0 in actual_scores:\n            # Any evaluable with a CRITICAL violation makes the project score 0\n            score = Score(self.min_score, self._badge(self.min_score))\n        elif len(actual_scores) == 0:\n            score = Score(self.max_score, self._badge(self.max_score))\n        else:\n            average_score = sum(actual_scores) / len(actual_scores)\n            score = Score(average_score, self._badge(average_score))\n        return score\n\n    def _badge(self, score: float) -&gt; str:\n        \"\"\"Compute the badge of a given score.\"\"\"\n        if score &gt;= self._config.badge_config.first.threshold:\n            return self._config.badge_config.first.icon\n        elif score &gt;= self._config.badge_config.second.threshold:\n            return self._config.badge_config.second.icon\n        elif score &gt;= self._config.badge_config.third.threshold:\n            return self._config.badge_config.third.icon\n        else:\n            return self._config.badge_config.wip.icon\n</code></pre>"},{"location":"reference/scoring/#dbt_score.scoring.Scorer.__init__","title":"<code>__init__(config)</code>","text":"<p>Create a Scorer object.</p> Source code in <code>src/dbt_score/scoring.py</code> <pre><code>def __init__(self, config: Config) -&gt; None:\n    \"\"\"Create a Scorer object.\"\"\"\n    self._config = config\n</code></pre>"},{"location":"reference/scoring/#dbt_score.scoring.Scorer.score_aggregate_evaluables","title":"<code>score_aggregate_evaluables(scores)</code>","text":"<p>Compute the score of a list of evaluables.</p> Source code in <code>src/dbt_score/scoring.py</code> <pre><code>def score_aggregate_evaluables(self, scores: list[Score]) -&gt; Score:\n    \"\"\"Compute the score of a list of evaluables.\"\"\"\n    actual_scores = [s.value for s in scores]\n    if 0.0 in actual_scores:\n        # Any evaluable with a CRITICAL violation makes the project score 0\n        score = Score(self.min_score, self._badge(self.min_score))\n    elif len(actual_scores) == 0:\n        score = Score(self.max_score, self._badge(self.max_score))\n    else:\n        average_score = sum(actual_scores) / len(actual_scores)\n        score = Score(average_score, self._badge(average_score))\n    return score\n</code></pre>"},{"location":"reference/scoring/#dbt_score.scoring.Scorer.score_evaluable","title":"<code>score_evaluable(evaluable_results)</code>","text":"<p>Compute the score of a given evaluable.</p> Source code in <code>src/dbt_score/scoring.py</code> <pre><code>def score_evaluable(self, evaluable_results: EvaluableResultsType) -&gt; Score:\n    \"\"\"Compute the score of a given evaluable.\"\"\"\n    rule_count = len(evaluable_results)\n\n    if rule_count == 0:\n        # No rule? No problem\n        score = self.max_score\n    elif any(\n        rule.severity == Severity.CRITICAL and isinstance(result, RuleViolation)\n        for rule, result in evaluable_results.items()\n    ):\n        # If there's a CRITICAL violation, the score is 0\n        score = self.min_score\n    else:\n        # Otherwise, the score is the weighted average (by severity) of the results\n        score = (\n            sum(\n                [\n                    # The more severe the violation, the more points are lost\n                    self.score_cardinality - rule.severity.value\n                    if isinstance(result, RuleViolation)  # Either 0/3, 1/3 or 2/3\n                    else self.score_cardinality  # 3/3\n                    for rule, result in evaluable_results.items()\n                ]\n            )\n            / (self.score_cardinality * rule_count)\n            * self.max_score\n        )\n\n    return Score(score, self._badge(score))\n</code></pre>"},{"location":"reference/formatters/","title":"Formatters","text":"<p>Formatters are used to output CLI results.</p>"},{"location":"reference/formatters/#dbt_score.formatters.Formatter","title":"<code>Formatter</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract class to define a formatter.</p> Source code in <code>src/dbt_score/formatters/__init__.py</code> <pre><code>class Formatter(ABC):\n    \"\"\"Abstract class to define a formatter.\"\"\"\n\n    def __init__(self, manifest_loader: ManifestLoader, config: Config):\n        \"\"\"Instantiate a formatter.\"\"\"\n        self._manifest_loader = manifest_loader\n        self._config = config\n\n    @abstractmethod\n    def evaluable_evaluated(\n        self, evaluable: Evaluable, results: EvaluableResultsType, score: Score\n    ) -&gt; None:\n        \"\"\"Callback when an evaluable item has been evaluated.\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def project_evaluated(self, score: Score) -&gt; None:\n        \"\"\"Callback when a project has been evaluated.\"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"reference/formatters/#dbt_score.formatters.Formatter.__init__","title":"<code>__init__(manifest_loader, config)</code>","text":"<p>Instantiate a formatter.</p> Source code in <code>src/dbt_score/formatters/__init__.py</code> <pre><code>def __init__(self, manifest_loader: ManifestLoader, config: Config):\n    \"\"\"Instantiate a formatter.\"\"\"\n    self._manifest_loader = manifest_loader\n    self._config = config\n</code></pre>"},{"location":"reference/formatters/#dbt_score.formatters.Formatter.evaluable_evaluated","title":"<code>evaluable_evaluated(evaluable, results, score)</code>  <code>abstractmethod</code>","text":"<p>Callback when an evaluable item has been evaluated.</p> Source code in <code>src/dbt_score/formatters/__init__.py</code> <pre><code>@abstractmethod\ndef evaluable_evaluated(\n    self, evaluable: Evaluable, results: EvaluableResultsType, score: Score\n) -&gt; None:\n    \"\"\"Callback when an evaluable item has been evaluated.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/formatters/#dbt_score.formatters.Formatter.project_evaluated","title":"<code>project_evaluated(score)</code>  <code>abstractmethod</code>","text":"<p>Callback when a project has been evaluated.</p> Source code in <code>src/dbt_score/formatters/__init__.py</code> <pre><code>@abstractmethod\ndef project_evaluated(self, score: Score) -&gt; None:\n    \"\"\"Callback when a project has been evaluated.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/formatters/human_readable_formatter/","title":"Human-readable formatter","text":"<p>Human readable formatter.</p>"},{"location":"reference/formatters/human_readable_formatter/#dbt_score.formatters.human_readable_formatter.HumanReadableFormatter","title":"<code>HumanReadableFormatter</code>","text":"<p>               Bases: <code>Formatter</code></p> <p>Formatter for human-readable messages in the terminal.</p> Source code in <code>src/dbt_score/formatters/human_readable_formatter.py</code> <pre><code>class HumanReadableFormatter(Formatter):\n    \"\"\"Formatter for human-readable messages in the terminal.\"\"\"\n\n    indent = \"    \"\n    label_ok = \"\\033[1;32mOK  \\033[0m\"\n    label_warning = \"\\033[1;33mWARN\\033[0m\"\n    label_error = \"\\033[1;31mERR \\033[0m\"\n\n    def __init__(self, *args: Any, **kwargs: Any):\n        \"\"\"Instantiate formatter.\"\"\"\n        super().__init__(*args, **kwargs)\n        self._failed_evaluables: list[tuple[Evaluable, Score]] = []\n\n    @staticmethod\n    def bold(text: str) -&gt; str:\n        \"\"\"Return text in bold.\"\"\"\n        return f\"\\033[1m{text}\\033[0m\"\n\n    @staticmethod\n    def pretty_name(evaluable: Evaluable) -&gt; str:\n        \"\"\"Return the pretty name for an evaluable.\"\"\"\n        match evaluable:\n            case Model():\n                return evaluable.name\n            case Source():\n                return evaluable.selector_name\n            case Snapshot():\n                return evaluable.name\n            case Exposure():\n                return evaluable.name\n            case Seed():\n                return evaluable.name\n            case _:\n                raise NotImplementedError\n\n    def evaluable_evaluated(\n        self, evaluable: Evaluable, results: EvaluableResultsType, score: Score\n    ) -&gt; None:\n        \"\"\"Callback when an evaluable item has been evaluated.\"\"\"\n        if evaluable_failed := score.value &lt; self._config.fail_any_item_under:\n            self._failed_evaluables.append((evaluable, score))\n        if (\n            evaluable_failed\n            or self._config.show == \"all\"\n            or (\n                self._config.show not in [\"failing-items\"]\n                and any(result is not None for result in results.values())\n            )\n        ):\n            resource_type = type(evaluable).__name__\n            name_formatted = f\"{resource_type}: {self.pretty_name(evaluable)}\"\n            header = (\n                f\"{score.badge} \"\n                f\"{self.bold(name_formatted)} (score: {score.rounded_value!s})\"\n            )\n\n            print(header)\n            for rule, result in results.items():\n                if result is None:\n                    if self._config.show in [\"all\"]:\n                        print(f\"{self.indent}{self.label_ok} {rule.source()}\")\n                elif isinstance(result, RuleViolation):\n                    print(\n                        f\"{self.indent}{self.label_warning} \"\n                        f\"({rule.severity.name.lower()}) {rule.source()}: \"\n                        f\"{result.message}\"\n                    )\n                else:\n                    print(\n                        f\"{self.indent}{self.label_error} {rule.source()}: {result!s}\"\n                    )\n            print()\n\n    def project_evaluated(self, score: Score) -&gt; None:\n        \"\"\"Callback when a project has been evaluated.\"\"\"\n        print(f\"Project score: {self.bold(str(score.rounded_value))} {score.badge}\")\n\n        if len(self._failed_evaluables) &gt; 0:\n            print()\n            print(\n                f\"Error: evaluable score too low, fail_any_item_under = \"\n                f\"{self._config.fail_any_item_under}\"\n            )\n            for evaluable, evaluable_score in self._failed_evaluables:\n                resource_type = type(evaluable)\n                print(\n                    f\"{resource_type.__name__} \"\n                    f\"{self.pretty_name(evaluable)} scored {evaluable_score.value}\"\n                )\n\n        elif score.value &lt; self._config.fail_project_under:\n            print()\n            print(\n                f\"Error: project score too low, fail_project_under = \"\n                f\"{self._config.fail_project_under}\"\n            )\n</code></pre>"},{"location":"reference/formatters/human_readable_formatter/#dbt_score.formatters.human_readable_formatter.HumanReadableFormatter.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Instantiate formatter.</p> Source code in <code>src/dbt_score/formatters/human_readable_formatter.py</code> <pre><code>def __init__(self, *args: Any, **kwargs: Any):\n    \"\"\"Instantiate formatter.\"\"\"\n    super().__init__(*args, **kwargs)\n    self._failed_evaluables: list[tuple[Evaluable, Score]] = []\n</code></pre>"},{"location":"reference/formatters/human_readable_formatter/#dbt_score.formatters.human_readable_formatter.HumanReadableFormatter.bold","title":"<code>bold(text)</code>  <code>staticmethod</code>","text":"<p>Return text in bold.</p> Source code in <code>src/dbt_score/formatters/human_readable_formatter.py</code> <pre><code>@staticmethod\ndef bold(text: str) -&gt; str:\n    \"\"\"Return text in bold.\"\"\"\n    return f\"\\033[1m{text}\\033[0m\"\n</code></pre>"},{"location":"reference/formatters/human_readable_formatter/#dbt_score.formatters.human_readable_formatter.HumanReadableFormatter.evaluable_evaluated","title":"<code>evaluable_evaluated(evaluable, results, score)</code>","text":"<p>Callback when an evaluable item has been evaluated.</p> Source code in <code>src/dbt_score/formatters/human_readable_formatter.py</code> <pre><code>def evaluable_evaluated(\n    self, evaluable: Evaluable, results: EvaluableResultsType, score: Score\n) -&gt; None:\n    \"\"\"Callback when an evaluable item has been evaluated.\"\"\"\n    if evaluable_failed := score.value &lt; self._config.fail_any_item_under:\n        self._failed_evaluables.append((evaluable, score))\n    if (\n        evaluable_failed\n        or self._config.show == \"all\"\n        or (\n            self._config.show not in [\"failing-items\"]\n            and any(result is not None for result in results.values())\n        )\n    ):\n        resource_type = type(evaluable).__name__\n        name_formatted = f\"{resource_type}: {self.pretty_name(evaluable)}\"\n        header = (\n            f\"{score.badge} \"\n            f\"{self.bold(name_formatted)} (score: {score.rounded_value!s})\"\n        )\n\n        print(header)\n        for rule, result in results.items():\n            if result is None:\n                if self._config.show in [\"all\"]:\n                    print(f\"{self.indent}{self.label_ok} {rule.source()}\")\n            elif isinstance(result, RuleViolation):\n                print(\n                    f\"{self.indent}{self.label_warning} \"\n                    f\"({rule.severity.name.lower()}) {rule.source()}: \"\n                    f\"{result.message}\"\n                )\n            else:\n                print(\n                    f\"{self.indent}{self.label_error} {rule.source()}: {result!s}\"\n                )\n        print()\n</code></pre>"},{"location":"reference/formatters/human_readable_formatter/#dbt_score.formatters.human_readable_formatter.HumanReadableFormatter.pretty_name","title":"<code>pretty_name(evaluable)</code>  <code>staticmethod</code>","text":"<p>Return the pretty name for an evaluable.</p> Source code in <code>src/dbt_score/formatters/human_readable_formatter.py</code> <pre><code>@staticmethod\ndef pretty_name(evaluable: Evaluable) -&gt; str:\n    \"\"\"Return the pretty name for an evaluable.\"\"\"\n    match evaluable:\n        case Model():\n            return evaluable.name\n        case Source():\n            return evaluable.selector_name\n        case Snapshot():\n            return evaluable.name\n        case Exposure():\n            return evaluable.name\n        case Seed():\n            return evaluable.name\n        case _:\n            raise NotImplementedError\n</code></pre>"},{"location":"reference/formatters/human_readable_formatter/#dbt_score.formatters.human_readable_formatter.HumanReadableFormatter.project_evaluated","title":"<code>project_evaluated(score)</code>","text":"<p>Callback when a project has been evaluated.</p> Source code in <code>src/dbt_score/formatters/human_readable_formatter.py</code> <pre><code>def project_evaluated(self, score: Score) -&gt; None:\n    \"\"\"Callback when a project has been evaluated.\"\"\"\n    print(f\"Project score: {self.bold(str(score.rounded_value))} {score.badge}\")\n\n    if len(self._failed_evaluables) &gt; 0:\n        print()\n        print(\n            f\"Error: evaluable score too low, fail_any_item_under = \"\n            f\"{self._config.fail_any_item_under}\"\n        )\n        for evaluable, evaluable_score in self._failed_evaluables:\n            resource_type = type(evaluable)\n            print(\n                f\"{resource_type.__name__} \"\n                f\"{self.pretty_name(evaluable)} scored {evaluable_score.value}\"\n            )\n\n    elif score.value &lt; self._config.fail_project_under:\n        print()\n        print(\n            f\"Error: project score too low, fail_project_under = \"\n            f\"{self._config.fail_project_under}\"\n        )\n</code></pre>"},{"location":"reference/formatters/json_formatter/","title":"JSON formatter","text":"<p>JSON formatter.</p> <p>Shape of the JSON output:</p> <pre><code>{\n    \"evaluables\": {\n        \"model.package.model_foo\": {\n            \"score\": 5.0,\n            \"badge\": \"\ud83e\udd48\",\n            \"pass\": true,\n            \"results\": {\n                \"rule1\": {\n                    \"result\": \"OK\",\n                    \"severity\": null\n                    \"message\": null\n                },\n                \"rule2\": {\n                    \"result\": \"WARN\",\n                    \"severity\": \"medium\",\n                    \"message\": \"Model lacks a description.\"\n                }\n            },\n            \"type\": \"model\"\n        },\n        \"model.package.model_bar\": {\n            \"score\": 0.0,\n            \"badge\": \"\ud83e\udd49\",\n            \"pass\": false,\n            \"results\": {\n                \"rule1\": {\n                    \"result\": \"ERR\",\n                    \"message\": \"Exception message\"\n                }\n            },\n            \"type\": \"model\"\n        },\n        \"source.package.source_name.source_baz\": {\n            \"score\": 10.0,\n            \"badge\": \"\ud83e\udd47\",\n            \"pass\": false,\n            \"results\": {\n                \"rule1\": {\n                    \"result\": \"ERR\",\n                    \"message\": \"Exception message\"\n                }\n            },\n            \"type\": \"source\"\n        }\n    },\n    \"project\": {\n        \"score\": 5.0,\n        \"badge\": \"\ud83e\udd48\",\n        \"pass\": false\n    }\n}\n</code></pre>"},{"location":"reference/formatters/json_formatter/#dbt_score.formatters.json_formatter.JSONFormatter","title":"<code>JSONFormatter</code>","text":"<p>               Bases: <code>Formatter</code></p> <p>Formatter for JSON output.</p> Source code in <code>src/dbt_score/formatters/json_formatter.py</code> <pre><code>class JSONFormatter(Formatter):\n    \"\"\"Formatter for JSON output.\"\"\"\n\n    def __init__(self, *args: Any, **kwargs: Any):\n        \"\"\"Instantiate formatter.\"\"\"\n        super().__init__(*args, **kwargs)\n        self.evaluable_results: dict[str, dict[str, Any]] = {}\n        self._project_results: dict[str, Any]\n\n    def evaluable_evaluated(\n        self, evaluable: Evaluable, results: EvaluableResultsType, score: Score\n    ) -&gt; None:\n        \"\"\"Callback when an evaluable item has been evaluated.\"\"\"\n        self.evaluable_results[evaluable.unique_id] = {\n            \"score\": score.value,\n            \"badge\": score.badge,\n            \"pass\": score.value &gt;= self._config.fail_any_item_under,\n            \"results\": {},\n            \"type\": type(evaluable).__name__.lower(),\n        }\n        for rule, result in results.items():\n            severity = rule.severity.name.lower()\n            if result is None:\n                self.evaluable_results[evaluable.unique_id][\"results\"][\n                    rule.source()\n                ] = {\n                    \"result\": \"OK\",\n                    \"severity\": severity,\n                    \"message\": None,\n                }\n            elif isinstance(result, RuleViolation):\n                self.evaluable_results[evaluable.unique_id][\"results\"][\n                    rule.source()\n                ] = {\n                    \"result\": \"WARN\",\n                    \"severity\": severity,\n                    \"message\": result.message,\n                }\n            else:\n                self.evaluable_results[evaluable.unique_id][\"results\"][\n                    rule.source()\n                ] = {\n                    \"result\": \"ERR\",\n                    \"severity\": severity,\n                    \"message\": str(result),\n                }\n\n    def project_evaluated(self, score: Score) -&gt; None:\n        \"\"\"Callback when a project has been evaluated.\"\"\"\n        self._project_results = {\n            \"score\": score.value,\n            \"badge\": score.badge,\n            \"pass\": score.value &gt;= self._config.fail_project_under,\n        }\n        document = {\n            \"evaluables\": self.evaluable_results,\n            \"project\": self._project_results,\n        }\n        print(json.dumps(document, indent=2, ensure_ascii=False))\n</code></pre>"},{"location":"reference/formatters/json_formatter/#dbt_score.formatters.json_formatter.JSONFormatter.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Instantiate formatter.</p> Source code in <code>src/dbt_score/formatters/json_formatter.py</code> <pre><code>def __init__(self, *args: Any, **kwargs: Any):\n    \"\"\"Instantiate formatter.\"\"\"\n    super().__init__(*args, **kwargs)\n    self.evaluable_results: dict[str, dict[str, Any]] = {}\n    self._project_results: dict[str, Any]\n</code></pre>"},{"location":"reference/formatters/json_formatter/#dbt_score.formatters.json_formatter.JSONFormatter.evaluable_evaluated","title":"<code>evaluable_evaluated(evaluable, results, score)</code>","text":"<p>Callback when an evaluable item has been evaluated.</p> Source code in <code>src/dbt_score/formatters/json_formatter.py</code> <pre><code>def evaluable_evaluated(\n    self, evaluable: Evaluable, results: EvaluableResultsType, score: Score\n) -&gt; None:\n    \"\"\"Callback when an evaluable item has been evaluated.\"\"\"\n    self.evaluable_results[evaluable.unique_id] = {\n        \"score\": score.value,\n        \"badge\": score.badge,\n        \"pass\": score.value &gt;= self._config.fail_any_item_under,\n        \"results\": {},\n        \"type\": type(evaluable).__name__.lower(),\n    }\n    for rule, result in results.items():\n        severity = rule.severity.name.lower()\n        if result is None:\n            self.evaluable_results[evaluable.unique_id][\"results\"][\n                rule.source()\n            ] = {\n                \"result\": \"OK\",\n                \"severity\": severity,\n                \"message\": None,\n            }\n        elif isinstance(result, RuleViolation):\n            self.evaluable_results[evaluable.unique_id][\"results\"][\n                rule.source()\n            ] = {\n                \"result\": \"WARN\",\n                \"severity\": severity,\n                \"message\": result.message,\n            }\n        else:\n            self.evaluable_results[evaluable.unique_id][\"results\"][\n                rule.source()\n            ] = {\n                \"result\": \"ERR\",\n                \"severity\": severity,\n                \"message\": str(result),\n            }\n</code></pre>"},{"location":"reference/formatters/json_formatter/#dbt_score.formatters.json_formatter.JSONFormatter.project_evaluated","title":"<code>project_evaluated(score)</code>","text":"<p>Callback when a project has been evaluated.</p> Source code in <code>src/dbt_score/formatters/json_formatter.py</code> <pre><code>def project_evaluated(self, score: Score) -&gt; None:\n    \"\"\"Callback when a project has been evaluated.\"\"\"\n    self._project_results = {\n        \"score\": score.value,\n        \"badge\": score.badge,\n        \"pass\": score.value &gt;= self._config.fail_project_under,\n    }\n    document = {\n        \"evaluables\": self.evaluable_results,\n        \"project\": self._project_results,\n    }\n    print(json.dumps(document, indent=2, ensure_ascii=False))\n</code></pre>"},{"location":"reference/formatters/manifest_formatter/","title":"Manifest formatter","text":"<p>Formatter for a manifest.json.</p>"},{"location":"reference/formatters/manifest_formatter/#dbt_score.formatters.manifest_formatter.ManifestFormatter","title":"<code>ManifestFormatter</code>","text":"<p>               Bases: <code>Formatter</code></p> <p>Formatter to generate manifest.json with score metadata.</p> Source code in <code>src/dbt_score/formatters/manifest_formatter.py</code> <pre><code>class ManifestFormatter(Formatter):\n    \"\"\"Formatter to generate manifest.json with score metadata.\"\"\"\n\n    def __init__(self, *args: Any, **kwargs: Any) -&gt; None:\n        \"\"\"Instantiate a manifest formatter.\"\"\"\n        self._evaluable_scores: dict[str, Score] = {}\n        super().__init__(*args, **kwargs)\n\n    def evaluable_evaluated(\n        self, evaluable: Evaluable, results: EvaluableResultsType, score: Score\n    ) -&gt; None:\n        \"\"\"Callback when an evaluable item has been evaluated.\"\"\"\n        self._evaluable_scores[evaluable.unique_id] = score\n\n    def project_evaluated(self, score: Score) -&gt; None:\n        \"\"\"Callback when a project has been evaluated.\"\"\"\n        manifest = copy.copy(self._manifest_loader.raw_manifest)\n        for evaluable_id, evaluable_score in self._evaluable_scores.items():\n            if evaluable_id.startswith(\"model\") or evaluable_id.startswith(\"snapshot\"):\n                nodes_manifest = manifest[\"nodes\"][evaluable_id]\n                nodes_manifest[\"meta\"][\"score\"] = evaluable_score.value\n                nodes_manifest[\"meta\"][\"badge\"] = evaluable_score.badge\n            if evaluable_id.startswith(\"source\"):\n                source_manifest = manifest[\"sources\"][evaluable_id]\n                source_manifest[\"meta\"][\"score\"] = evaluable_score.value\n                source_manifest[\"meta\"][\"badge\"] = evaluable_score.badge\n            if evaluable_id.startswith(\"exposure\"):\n                exposure_manifest = manifest[\"exposures\"][evaluable_id]\n                exposure_manifest[\"meta\"][\"score\"] = evaluable_score.value\n                exposure_manifest[\"meta\"][\"badge\"] = evaluable_score.badge\n        print(json.dumps(manifest, indent=2))\n</code></pre>"},{"location":"reference/formatters/manifest_formatter/#dbt_score.formatters.manifest_formatter.ManifestFormatter.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Instantiate a manifest formatter.</p> Source code in <code>src/dbt_score/formatters/manifest_formatter.py</code> <pre><code>def __init__(self, *args: Any, **kwargs: Any) -&gt; None:\n    \"\"\"Instantiate a manifest formatter.\"\"\"\n    self._evaluable_scores: dict[str, Score] = {}\n    super().__init__(*args, **kwargs)\n</code></pre>"},{"location":"reference/formatters/manifest_formatter/#dbt_score.formatters.manifest_formatter.ManifestFormatter.evaluable_evaluated","title":"<code>evaluable_evaluated(evaluable, results, score)</code>","text":"<p>Callback when an evaluable item has been evaluated.</p> Source code in <code>src/dbt_score/formatters/manifest_formatter.py</code> <pre><code>def evaluable_evaluated(\n    self, evaluable: Evaluable, results: EvaluableResultsType, score: Score\n) -&gt; None:\n    \"\"\"Callback when an evaluable item has been evaluated.\"\"\"\n    self._evaluable_scores[evaluable.unique_id] = score\n</code></pre>"},{"location":"reference/formatters/manifest_formatter/#dbt_score.formatters.manifest_formatter.ManifestFormatter.project_evaluated","title":"<code>project_evaluated(score)</code>","text":"<p>Callback when a project has been evaluated.</p> Source code in <code>src/dbt_score/formatters/manifest_formatter.py</code> <pre><code>def project_evaluated(self, score: Score) -&gt; None:\n    \"\"\"Callback when a project has been evaluated.\"\"\"\n    manifest = copy.copy(self._manifest_loader.raw_manifest)\n    for evaluable_id, evaluable_score in self._evaluable_scores.items():\n        if evaluable_id.startswith(\"model\") or evaluable_id.startswith(\"snapshot\"):\n            nodes_manifest = manifest[\"nodes\"][evaluable_id]\n            nodes_manifest[\"meta\"][\"score\"] = evaluable_score.value\n            nodes_manifest[\"meta\"][\"badge\"] = evaluable_score.badge\n        if evaluable_id.startswith(\"source\"):\n            source_manifest = manifest[\"sources\"][evaluable_id]\n            source_manifest[\"meta\"][\"score\"] = evaluable_score.value\n            source_manifest[\"meta\"][\"badge\"] = evaluable_score.badge\n        if evaluable_id.startswith(\"exposure\"):\n            exposure_manifest = manifest[\"exposures\"][evaluable_id]\n            exposure_manifest[\"meta\"][\"score\"] = evaluable_score.value\n            exposure_manifest[\"meta\"][\"badge\"] = evaluable_score.badge\n    print(json.dumps(manifest, indent=2))\n</code></pre>"},{"location":"rules/filters/","title":"Rule filters","text":"<p>Rule filters.</p>"},{"location":"rules/filters/#dbt_score.rules.filters.is_table","title":"<code>is_table(model)</code>","text":"<p>Models that are tables.</p> Source code in <code>src/dbt_score/rules/filters.py</code> <pre><code>@rule_filter\ndef is_table(model: Model) -&gt; bool:\n    \"\"\"Models that are tables.\"\"\"\n    return model.config.get(\"materialized\") in {\"table\", \"incremental\"}\n</code></pre>"},{"location":"rules/generic/","title":"Generic","text":""},{"location":"rules/generic/#columns_have_description","title":"<code>columns_have_description</code>","text":"<p>All columns of a model should have a description.</p> Source code <pre><code>@rule\ndef columns_have_description(model: Model) -&gt; RuleViolation | None:\n    \"\"\"All columns of a model should have a description.\"\"\"\n    invalid_column_names = [\n        column.name for column in model.columns if not column.description\n    ]\n    if invalid_column_names:\n        max_length = 60\n        message = f\"Columns lack a description: {', '.join(invalid_column_names)}.\"\n        if len(message) &gt; max_length:\n            message = f\"{message[:max_length]}\u2026\"\n        return RuleViolation(message=message)\n</code></pre>"},{"location":"rules/generic/#default-configuration","title":"Default configuration","text":"pyproject.toml<pre><code>[tool.dbt-score.rules.\"dbt_score.rules.generic.columns_have_description\"]\nseverity = 2\n</code></pre>"},{"location":"rules/generic/#has_description","title":"<code>has_description</code>","text":"<p>A model should have a description.</p> Source code <pre><code>@rule\ndef has_description(model: Model) -&gt; RuleViolation | None:\n    \"\"\"A model should have a description.\"\"\"\n    if not model.description:\n        return RuleViolation(message=\"Model lacks a description.\")\n</code></pre>"},{"location":"rules/generic/#default-configuration_1","title":"Default configuration","text":"pyproject.toml<pre><code>[tool.dbt-score.rules.\"dbt_score.rules.generic.has_description\"]\nseverity = 2\n</code></pre>"},{"location":"rules/generic/#has_example_sql","title":"<code>has_example_sql</code>","text":"<p>The documentation of a model should have an example query.</p> Source code <pre><code>@rule(severity=Severity.LOW)\ndef has_example_sql(model: Model) -&gt; RuleViolation | None:\n    \"\"\"The documentation of a model should have an example query.\"\"\"\n    if model.language == \"sql\":\n        if \"```sql\" not in (model.description or \"\"):\n            return RuleViolation(\n                \"The model description does not include an example SQL query.\"\n            )\n</code></pre>"},{"location":"rules/generic/#default-configuration_2","title":"Default configuration","text":"pyproject.toml<pre><code>[tool.dbt-score.rules.\"dbt_score.rules.generic.has_example_sql\"]\nseverity = 1\n</code></pre>"},{"location":"rules/generic/#has_no_unused_is_incremental","title":"<code>has_no_unused_is_incremental</code>","text":"<p>Non-incremental model does not make use of is_incremental().</p> Source code <pre><code>@rule(rule_filters={is_table()})\ndef has_no_unused_is_incremental(model: Model) -&gt; RuleViolation | None:\n    \"\"\"Non-incremental model does not make use of is_incremental().\"\"\"\n    if (\n        model.config.get(\"materialized\") != \"incremental\"\n        and \"is_incremental()\" in model.raw_code\n    ):\n        return RuleViolation(\"Non-incremental model makes use of is_incremental().\")\n</code></pre>"},{"location":"rules/generic/#default-configuration_3","title":"Default configuration","text":"pyproject.toml<pre><code>[tool.dbt-score.rules.\"dbt_score.rules.generic.has_no_unused_is_incremental\"]\nseverity = 2\n</code></pre>"},{"location":"rules/generic/#has_owner","title":"<code>has_owner</code>","text":"<p>A model should have an owner.</p> Source code <pre><code>@rule\ndef has_owner(model: Model) -&gt; RuleViolation | None:\n    \"\"\"A model should have an owner.\"\"\"\n    if not model.meta.get(\"owner\"):\n        return RuleViolation(message=\"Model lacks an owner.\")\n</code></pre>"},{"location":"rules/generic/#default-configuration_4","title":"Default configuration","text":"pyproject.toml<pre><code>[tool.dbt-score.rules.\"dbt_score.rules.generic.has_owner\"]\nseverity = 2\n</code></pre>"},{"location":"rules/generic/#has_uniqueness_test","title":"<code>has_uniqueness_test</code>","text":"<p>Model has uniqueness test for primary key.</p> Source code <pre><code>@rule(rule_filters={is_table()})\ndef has_uniqueness_test(model: Model) -&gt; RuleViolation | None:\n    \"\"\"Model has uniqueness test for primary key.\"\"\"\n    # ruff: noqa: C901 [too-complex]\n\n    # Single-column PK\n    for column in model.columns:\n        for column_constraint in column.constraints:\n            if column_constraint.type == \"primary_key\":\n                for data_test in column.tests:\n                    if data_test.type == \"unique\":\n                        return None\n                return RuleViolation(\n                    f\"No unique constraint defined on PK column {column.name}.\"\n                )\n\n    # Composite PK\n    pk_columns: list[str] = []\n    for model_constraint in model.constraints:\n        if model_constraint.type == \"primary_key\":\n            pk_columns = model_constraint.columns or []\n            break\n\n    if not pk_columns:  # No PK, no need for uniqueness test\n        return None\n\n    for data_test in model.tests:\n        if data_test.type == \"unique_combination_of_columns\":\n            if set(data_test.kwargs.get(\"combination_of_columns\")) == set(pk_columns):  # type: ignore\n                return None\n    return RuleViolation(\n        f\"No uniqueness test defined and matching PK {','.join(pk_columns)}.\"\n    )\n</code></pre>"},{"location":"rules/generic/#default-configuration_5","title":"Default configuration","text":"pyproject.toml<pre><code>[tool.dbt-score.rules.\"dbt_score.rules.generic.has_uniqueness_test\"]\nseverity = 2\n</code></pre>"},{"location":"rules/generic/#seed_columns_have_description","title":"<code>seed_columns_have_description</code>","text":"<p>All columns of a seed should have a description.</p> Source code <pre><code>@rule\ndef seed_columns_have_description(seed: Seed) -&gt; RuleViolation | None:\n    \"\"\"All columns of a seed should have a description.\"\"\"\n    invalid_column_names = [\n        column.name for column in seed.columns if not column.description\n    ]\n    if invalid_column_names:\n        max_length = 60\n        message = f\"Columns lack a description: {', '.join(invalid_column_names)}.\"\n        if len(message) &gt; max_length:\n            message = f\"{message[:max_length]}\u2026\"\n        return RuleViolation(message=message)\n</code></pre>"},{"location":"rules/generic/#default-configuration_6","title":"Default configuration","text":"pyproject.toml<pre><code>[tool.dbt-score.rules.\"dbt_score.rules.generic.seed_columns_have_description\"]\nseverity = 2\n</code></pre>"},{"location":"rules/generic/#seed_has_description","title":"<code>seed_has_description</code>","text":"<p>A seed should have a description.</p> Source code <pre><code>@rule\ndef seed_has_description(seed: Seed) -&gt; RuleViolation | None:\n    \"\"\"A seed should have a description.\"\"\"\n    if not seed.description:\n        return RuleViolation(message=\"Seed lacks a description.\")\n</code></pre>"},{"location":"rules/generic/#default-configuration_7","title":"Default configuration","text":"pyproject.toml<pre><code>[tool.dbt-score.rules.\"dbt_score.rules.generic.seed_has_description\"]\nseverity = 2\n</code></pre>"},{"location":"rules/generic/#seed_has_owner","title":"<code>seed_has_owner</code>","text":"<p>A seed should have an owner.</p> Source code <pre><code>@rule\ndef seed_has_owner(seed: Seed) -&gt; RuleViolation | None:\n    \"\"\"A seed should have an owner.\"\"\"\n    meta = seed.config.get(\"meta\", {})\n    if not meta.get(\"owner\"):\n        return RuleViolation(message=\"Seed lacks an owner.\")\n</code></pre>"},{"location":"rules/generic/#default-configuration_8","title":"Default configuration","text":"pyproject.toml<pre><code>[tool.dbt-score.rules.\"dbt_score.rules.generic.seed_has_owner\"]\nseverity = 2\n</code></pre>"},{"location":"rules/generic/#single_column_uniqueness_at_column_level","title":"<code>single_column_uniqueness_at_column_level</code>","text":"<p>Single-column uniqueness test must be defined as a column test.</p> Source code <pre><code>@rule(rule_filters={is_table()})\ndef single_column_uniqueness_at_column_level(model: Model) -&gt; RuleViolation | None:\n    \"\"\"Single-column uniqueness test must be defined as a column test.\"\"\"\n    for data_test in model.tests:\n        if data_test.type == \"unique_combination_of_columns\":\n            if len(data_test.kwargs.get(\"combination_of_columns\", [])) == 1:\n                return RuleViolation(\n                    \"Single-column uniqueness test must be defined as a column test.\"\n                )\n</code></pre>"},{"location":"rules/generic/#default-configuration_9","title":"Default configuration","text":"pyproject.toml<pre><code>[tool.dbt-score.rules.\"dbt_score.rules.generic.single_column_uniqueness_at_column_level\"]\nseverity = 2\n</code></pre>"},{"location":"rules/generic/#single_pk_defined_at_column_level","title":"<code>single_pk_defined_at_column_level</code>","text":"<p>Single-column PK must be defined as a column constraint.</p> Source code <pre><code>@rule(rule_filters={is_table()})\ndef single_pk_defined_at_column_level(model: Model) -&gt; RuleViolation | None:\n    \"\"\"Single-column PK must be defined as a column constraint.\"\"\"\n    for constraint in model.constraints:\n        if constraint.type == \"primary_key\":\n            if constraint.columns is not None and len(constraint.columns) == 1:\n                return RuleViolation(\n                    f\"Single-column PK {constraint.columns[0]} must be defined as a \"\n                    f\"column constraint.\"\n                )\n</code></pre>"},{"location":"rules/generic/#default-configuration_10","title":"Default configuration","text":"pyproject.toml<pre><code>[tool.dbt-score.rules.\"dbt_score.rules.generic.single_pk_defined_at_column_level\"]\nseverity = 2\n</code></pre>"},{"location":"rules/generic/#snapshot_has_strategy","title":"<code>snapshot_has_strategy</code>","text":"<p>A snapshot should have a strategy.</p> Source code <pre><code>@rule\ndef snapshot_has_strategy(snapshot: Snapshot) -&gt; RuleViolation | None:\n    \"\"\"A snapshot should have a strategy.\"\"\"\n    if not snapshot.config.get(\"strategy\"):\n        return RuleViolation(message=\"Snapshot lacks a strategy.\")\n</code></pre>"},{"location":"rules/generic/#default-configuration_11","title":"Default configuration","text":"pyproject.toml<pre><code>[tool.dbt-score.rules.\"dbt_score.rules.generic.snapshot_has_strategy\"]\nseverity = 2\n</code></pre>"},{"location":"rules/generic/#snapshot_has_unique_key","title":"<code>snapshot_has_unique_key</code>","text":"<p>A snapshot should have a unique key.</p> Source code <pre><code>@rule\ndef snapshot_has_unique_key(snapshot: Snapshot) -&gt; RuleViolation | None:\n    \"\"\"A snapshot should have a unique key.\"\"\"\n    if not snapshot.config.get(\"unique_key\"):\n        return RuleViolation(message=\"Snapshot lacks a unique key.\")\n</code></pre>"},{"location":"rules/generic/#default-configuration_12","title":"Default configuration","text":"pyproject.toml<pre><code>[tool.dbt-score.rules.\"dbt_score.rules.generic.snapshot_has_unique_key\"]\nseverity = 2\n</code></pre>"},{"location":"rules/generic/#sql_has_reasonable_number_of_lines","title":"<code>sql_has_reasonable_number_of_lines</code>","text":"<p>The SQL query of a model should not be too long.</p> Source code <pre><code>@rule\ndef sql_has_reasonable_number_of_lines(\n    model: Model, max_lines: int = 200\n) -&gt; RuleViolation | None:\n    \"\"\"The SQL query of a model should not be too long.\"\"\"\n    count_lines = len(model.raw_code.split(\"\\n\"))\n    if count_lines &gt; max_lines:\n        return RuleViolation(\n            message=f\"SQL query too long: {count_lines} lines (&gt; {max_lines}).\"\n        )\n</code></pre>"},{"location":"rules/generic/#default-configuration_13","title":"Default configuration","text":"pyproject.toml<pre><code>[tool.dbt-score.rules.\"dbt_score.rules.generic.sql_has_reasonable_number_of_lines\"]\nseverity = 2\nmax_lines = 200\n</code></pre>"}]}